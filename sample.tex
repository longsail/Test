%% 使用 njuthesis 文档类生成南京大学学位论文的示例文档
%%
%% 作者：胡海星，starfish (at) gmail (dot) com
%% 项目主页: http://haixing-hu.github.io/nju-thesis/
%%
%% 本样例文档中用到了吕琦同学的博士论文的提高和部分内容，在此对他表示感谢。
%%
\documentclass[phd]{njuthesis}
%% njuthesis 文档类的可选参数有：
%%   nobackinfo 取消封二页导师签名信息。注意，按照南大的规定，是需要签名页的。
%%   phd/master/bachelor 选择博士/硕士/学士论文

% 使用 blindtext 宏包自动生成章节文字
% 这仅仅是用于生成样例文档，正式论文中一般用不到该宏包
\usepackage[math]{blindtext}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置《国家图书馆封面》的内容，仅博士论文才需要填写

% 设置论文按照《中国图书资料分类法》的分类编号
\classification{0175.2}
% 论文的密级。需按照GB/T 7156-2003标准进行设置。预定义的值包括：
% - \openlevel，表示公开级：此级别的文献可在国内外发行和交换。
% - \controllevel，表示限制级：此级别的文献内容不涉及国家秘密，但在一定时间内
%   限制其交流和使用范围。
% - \confidentiallevel，表示秘密级：此级别的文献内容涉及一般国家秘密。
% - \clasifiedlevel，表示机密级：此级别的文献内容涉及重要的国家秘密 。
% - \mostconfidentiallevel，表示绝密级：此级别的文献内容涉及最重要的国家秘密。
% 此属性可选，默认为\openlevel，即公开级。
\securitylevel{\controllevel}
% 设置论文按照《国际十进分类法UDC》的分类编号
% 该编号可在下述网址查询：http://www.udcc.org/udcsummary/php/index.php?lang=chi
\udc{004.72}
% 国家图书馆封面上的论文标题第一行，不可换行。此属性可选，默认值为通过\title设置的标题。
\nlctitlea{数据中心}
% 国家图书馆封面上的论文标题第二行，不可换行。此属性可选，默认值为空白。
\nlctitleb{网络模型研究}
% 国家图书馆封面上的论文标题第三行，不可换行。此属性可选，默认值为空白。
\nlctitlec{}
% 导师的单位名称及地址
\supervisorinfo{南京大学计算机科学与技术系~~南京市汉口路22号~~210093}
% 答辩委员会主席
\chairman{张三丰~~教授}
% 第一位评阅人
\reviewera{阳顶天~~教授}
% 第二位评阅人
\reviewerb{张无忌~~副教授}
% 第三位评阅人
\reviewerc{黄裳~~教授}
% 第四位评阅人
\reviewerd{郭靖~~研究员}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文封面

% 论文标题，不可换行
\title{数据中心网络模型研究}
% 论文作者姓名
\author{韦小宝}
% 论文作者联系电话
\telphone{13671413272}
% 论文作者电子邮件地址
\email{xiaobao.wei@gmail.com}
% 论文作者学生证号
\studentnum{MG0033011}
% 论文作者入学年份（年级）
\grade{2010}
% 导师姓名职称
\supervisor{陈近南~~教授}
% 导师的联系电话
\supervisortelphone{13671607471}
% 论文作者的学科与专业方向
\major{计算机软件与理论}
% 论文作者的研究方向
\researchfield{计算机网络与信息安全}
% 论文作者所在院系的中文名称
\department{计算机科学与技术系}
% 论文作者所在学校或机构的名称。此属性可选，默认值为``南京大学''。
\institute{南京大学}
% 论文的提交日期，需设置年、月、日。vvvvvv
\submitdate{2013年5月10日}
% 论文的答辩日期，需设置年、月、日。
\defenddate{2013年6月1日}
% 论文的定稿日期，需设置年、月、日。此属性可选，默认值为最后一次编译时的日期，精确到日。
%% \date{2013年5月1日}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文封面

% 论文的英文标题，不可换行
\englishtitle{A Research on Network Infrastructures for Data Centers}
% 论文作者姓名的拼音
\englishauthor{WEI Xiao-Bao}
% 导师姓名职称的英文
\englishsupervisor{Professor CHEN Jin-Nan}
% 论文作者学科与专业的英文名
\englishmajor{Computer Software and Theory}
% 论文作者所在院系的英文名称
\englishdepartment{Department of Computer Science and Technology}
% 论文作者所在学校或机构的英文名称。此属性可选，默认值为``Nanjing University''。
\englishinstitute{Nanjing University}
% 论文完成日期的英文形式，它将出现在英文封面下方。需设置年、月、日。日期格式使用美国的日期
% 格式，即``Month day, year''，其中``Month''为月份的英文名全称，首字母大写；``day''为
% 该月中日期的阿拉伯数字表示；``year''为年份的四位阿拉伯数字表示。此属性可选，默认值为最后
% 一次编译时的日期。
\englishdate{May 1, 2013}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文摘要

% 设置中文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\title|命令所设置的论文标题
% \abstracttitlea{数据中心网络模型研究}
% 设置中文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
% \abstracttitleb{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文摘要

% 设置英文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\englishtitle|命令所设置的论文标题
\englishabstracttitlea{A Research on Network Infrastructures}
% 设置英文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
\englishabstracttitleb{for Data Centers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 制作国家图书馆封面（博士学位论文才需要）
\makenlctitle
% 制作中文封面
\maketitle
% 制作英文封面
\makeenglishtitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始前言部分
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的中文摘要
\begin{abstract}
复杂网络的研究可上溯到20世纪60年代对ER网络的研究。90年后代随着Internet
的发展，以及对人类社会、通信网络、生物网络、社交网络等各领域研究的深入，
发现了小世界网络和无尺度现象等普适现象与方法。对复杂网络的定性定量的科
学理解和分析，已成为如今网络时代科学研究的一个重点课题。

在此背景下，由于云计算时代的到来，本文针对面向云计算的数据中心网络基础
设施设计中的若干问题，进行了几方面的研究。………………
% 中文关键词。关键词之间用中文全角分号隔开，末尾无标点符号。
\keywords{小世界理论；网络模型；数据中心}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的英文摘要
\begin{englishabstract}
\blindtext
% 英文关键词。关键词之间用英文半角逗号隔开，末尾无符号。
\englishkeywords{Small World, Network Model, Data Center}
\end{englishabstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的前言，应放在目录之前，中英文摘要之后
%
\begin{preface}

复杂网络的研究可上溯到20世纪60年代对ER网络的研究。90年后代随着Internet
的发展，以及对人类社会、通信网络、生物网络、社交网络等各领域研究的深入，
发现了小世界网络和无尺度现象等普适现象与方法。对复杂网络的定性定量的科
学理解和分析，已成为如今网络时代科学研究的一个重点课题。

在此背景下，由于云计算时代的到来，本文针对面向云计算的数据中心网络基础
设施设计中的若干问题，进行了几方面的研究。本文的创造性研究成果主要如下
几方面：

\begin{enumerate}
\item 基于簇划分的思想，提出并设计了WarpNet网络模型。该网络模型基于随机
  散列，以节点微路由链接多种散列分布，实现网络互联。并对网络的带宽等指
  标进行理论分析并给出定量描述。最后对比了理论分析、仿真测试结果，并在
  实际物理环境中进系真实部署，通过6节点的小规模实验以及1000节点虚拟机的
  大规模实验，表明该模型的理论分析、仿真测试与实际实验吻合，并在网络性
  能、容错能力、伸缩性灵活性方面得到较大提升。
\item 提出DS小世界模型并构造SIDN网络，解决了把小世界理论应用于数据中心
  网络布局构建中的最大度限制问题。分析了在带有最大度限制约束下，所构成
  网络的平均直径、网络总带宽、对故障的容错能力等各项网络参数。理论分析
  与仿真实验证明，SIDN网络具有很好的扩展能力，网络总带宽与网络规模成近
  似线性增长的关系；具有很强的容错能力，链路损坏与节点损坏几乎无法破坏
  网络的联通性，故障率对网络性能的影响与破坏节点/链路占总资源比率线性相
  关。
\item 分析了无尺度网络在数据中心网络构建应用中的理论方面问题。在引入节
  点最大度限制之后，给出无尺度网络的各项网络参数。并进一步分析了交换机
  节点以及计算节点两种角色在不同比率的组合下对网络性能的影响，给出最高
  性价比的比率参数。最后通过理论分析与仿真实验证明，在引入了无尺度现象
  之后，提高了网络的聚类系数，从而显著的提升了网络的性能。

\item 针对网络模型研究这一类工作的共性，设计构造通用验证平台系统。以海
  量虚拟机和虚拟分布式交换机的形式，实现了基于少量物理节点，对大规模节
  点的模拟。其模拟运行的过程与真实运行在实现层面完全一致，运行的结果与
  真实环境线性相关。除为本文所涉若干网络模型提供验证外，可进一步推广到
  更为广泛的领域，为各种网络模型及路由算法的研究工作，提供分析、指导与
  验证。
\end{enumerate}

复杂网络的研究可上溯到20世纪60年代对ER网络的研究。90年后代随着Internet
的发展，以及对人类社会、通信网络、生物网络、社交网络等各领域研究的深入，
发现了小世界网络和无尺度现象等普适现象与方法。对复杂网络的定性定量的科
学理解和分析，已成为如今网络时代科学研究的一个重点课题。

在此背景下，由于云计算时代的到来，本文针对面向云计算的数据中心网络基础
设施设计中的若干问题，进行了几方面的研究。本文的创造性研究成果主要如下
几方面：

\begin{enumerate}
\item 基于簇划分的思想，提出并设计了WarpNet网络模型。该网络模型基于随机
  散列，以节点微路由链接多种散列分布，实现网络互联。并对网络的带宽等指
  标进行理论分析并给出定量描述。最后对比了理论分析、仿真测试结果，并在
  实际物理环境中进系真实部署，通过6节点的小规模实验以及1000节点虚拟机的
  大规模实验，表明该模型的理论分析、仿真测试与实际实验吻合，并在网络性
  能、容错能力、伸缩性灵活性方面得到较大提升。
\item 提出DS小世界模型并构造SIDN网络，解决了把小世界理论应用于数据中心
  网络布局构建中的最大度限制问题。分析了在带有最大度限制约束下，所构成
  网络的平均直径、网络总带宽、对故障的容错能力等各项网络参数。理论分析
  与仿真实验证明，SIDN网络具有很好的扩展能力，网络总带宽与网络规模成近
  似线性增长的关系；具有很强的容错能力，链路损坏与节点损坏几乎无法破坏
  网络的联通性，故障率对网络性能的影响与破坏节点/链路占总资源比率线性相
  关。
\item 分析了无尺度网络在数据中心网络构建应用中的理论方面问题。在引入节
  点最大度限制之后，给出无尺度网络的各项网络参数。并进一步分析了交换机
  节点以及计算节点两种角色在不同比率的组合下对网络性能的影响，给出最高
  性价比的比率参数。最后通过理论分析与仿真实验证明，在引入了无尺度现象
  之后，提高了网络的聚类系数，从而显著的提升了网络的性能。

\item 针对网络模型研究这一类工作的共性，设计构造通用验证平台系统。以海
  量虚拟机和虚拟分布式交换机的形式，实现了基于少量物理节点，对大规模节
  点的模拟。其模拟运行的过程与真实运行在实现层面完全一致，运行的结果与
  真实环境线性相关。除为本文所涉若干网络模型提供验证外，可进一步推广到
  更为广泛的领域，为各种网络模型及路由算法的研究工作，提供分析、指导与
  验证。
\end{enumerate}

\vspace{1cm}
\begin{flushright}
韦小宝\\
2013年夏于南京大学
\end{flushright}

\end{preface}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成论文目次
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成插图清单。如无需插图清单则可注释掉下述语句。
\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成附表清单。如无需附表清单则可注释掉下述语句。
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始正文部分
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《绪论》作为第一章

\chapter{绪论}\label{chapter_introduction}
\section{研究背景}

我的毕业设计focus在机器学习算法在短文本上的应用，随着社交网络的兴起，类似于weibo或者dialogue query这种短文本越来越多的出现在人们的生活当中，对于研究者来说面临着以下诸多挑战，首先随着新浪微博以及cortana在的大量使用，每天产生巨大的数据，我们如何去获取这些数据，另一方面，无论是微博还是dialogue query本身的长度非常短，传统的文本处理方法是否依然适用。
在这里我们期望将传统的文本处理方法在短文本应用起来。这里我将选择三种在短文本中的应用去介绍，他们是关键词短语提取，domain classification以及incremental learning。对于三种应用，我将从研究背景，相关研究，算法介绍，实验设计以及未来可能改进的地方等等加以介绍。
本文的结构组织如下，第一章主要介绍基于LDA和PageRank的方法在关键词短语提取中的应用，这里我们将从数据爬取，算法设计，结果展示三个方面来介绍。其中在算法设计过程中我们借鉴了LDA以及pagerank，我们会对此两种算法做简单的介绍。在结果展示中我们为了呈现更直观的结果，我们将结果可视化出来。
第二章主要介绍domain classification using slot feature，首先我们将会阐述何为domain classification，何为slot feature，这一章我们将从算法设计，以及实验结果两方面来展示，在算法设计过程中我们使用SVM算法以及CRF算法，所以我们会对SVM以及CRF算法做详细的介绍。
第三章介绍incremental learning的知识，首先我们将会阐述何为incremental learning。Incremental learning与domain classification的联系以及我们为什么需要incremental learning。在incremental learning的实验中我们使用了两种优化算法，我们会首先阐述优化算法在机器学习中的意义，对于使用的两种优化算法我们将会做详细的介绍，最后我们总结了实验结果。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{基于LDA和PageRank的关键词短语提取算法}\label{chapter_smallworld}
\section{研究背景}

近些年来随着社交网络的兴起，对于社交网络的研究以及基于社交网络的应用越来越多，在中国新浪微博的用户数量活跃度最高，随着新浪微博的使用越来越来普及，新浪微博每时每刻都在产生大量的数据，如何利用这些数据是一件非常有意义的事情。当然现在基于新浪微博上的上的应用已经非常丰富了，例如政府可以对于社会热点事件进行舆情分析，企业主可以实时获取自己的产品在微博上的反馈，广告主可以根据用户过去的浏览以及关注历史向用户定向推荐广告。然而无论是舆情分析，产品反馈亦或是广告推荐，关键词提取对于这些应用来说都会有很大的帮助。关键词的描述能力已经很强并且已经有了很多成熟的关键词提取方法，我们认为关键词短语带有更多的信息并且描述能力更好。因此我们希望可以基于某个关键词，首先在新浪微博上爬取一批数量的微博，然后在爬取到的微博上提取出来相应的关键词短语，这些关键词短语可以用于舆情分析，产品反馈，广告推荐，同时这些关键词短语还可以尝试用来做微博上的的分类，推荐以及用于预测微博上的热点事件发生等等应用。

对于长文本来说，文本处理相对容易并且长文本上的关键词短语的提取已经有了不错的研究。对于新浪微博，将会遇到下面的问题，微博长度比传统的文本短，有一部分微博没有有用的信息，微博上的主题更加多样化。我们希望可以利用长文本分析的技术同时结合短文本的特点在微博上面实现关键词短语算法。同时伴随着可视化技术的发展以及可视化可以给用户带来直观的感受，我们将自己处理的结果使用可视化工具呈现出来。

本章接下来的组织如下，首先我们将阐述关键词短语提取的相关工作TFIDF，TEXTRANK等关键词提取方法。其次我们将详细介绍基于LDA和PageRank的关键词短语提取算法。然后简略的介绍系统架构，接着描述实验设计以及结果，最后总结本章。

\section{关键词短语提取相关工作}

在介绍关键词短语之前首先介绍关键词提取的概念。关键词提取指的是从一篇或者一批文档中提取出来的词语，这些词语可以概括文档的中心思想，被广泛的用在建立索引，文档概要等应用中。关键词短语指的是由两个及以上的词语组成基于关键词提取的方法，不再关注提取单一的词语而是将关注点放在提取名词短语上。关键词短语长度更长，具有比关键词更强的描述能力。

关键词短语的提取的方法可以分为有监督学习的方法以及无监督学习的方法，接下来将简略的介绍有监督学习以及无监督学习的概念。有监督学习是需要人工标注一批数据，然后将数据划分成训练集以及测试集，利用机器学习的算法在训练集上训练出模型，在测试集上测试模型的效果进而反馈给训练方法修改模型参数，使得最终的模型具有很好的泛化能力。有监督学习中最普遍的一类机器学习算法就是分类以及回归算法。无监督学习指的是不需要人工标注数据，让计算机自己去学习，无监督学习算法中最普遍的一类算法就是聚类算法。
在关键词短语提取中，有监督学习的方法将关键词提取看作是一个分类的任务，通过使用语料集训练模型来判断候选的关键词短语是否是真正的关键词短语，在无监督方法中基于图的排序算法是学术上效果最好的方法[copy]。在当今时代，互联网发展迅速可以很容易方便的获取大量的数据，如果对大规模的数据进行标注，需要付出很大的人工成本以及时间代价，在实际工作中几乎是不可能完成的，所以有监督的方法不太实用，因此基于无监督学习的方法越来越受到重视。接下来我们将利用这篇paper$"Automatic Keyphrase Extraction via Topic Decomposition"$里面提到的算法，在新浪微博的语料上尝试提取关键词。

本章接下来的组织如下，首先介绍基于TFIDF模型的关键词短语提取算法，接着介绍基于TextRank模型的关键词短语提取算法。

\subsection{基于TFIDF的关键词短语提取方法}

TFIDF是一种统计方法，在信息检索中使用比较广泛，可以用来评估一个词对于一个文件集合的重要程度。TFIDF主要思想就是，如果某个词在一篇文档中出现的频率高，在其它文档中出现的次数比较少，就认为这个词具有很好的区分能力。TFIDF中TF（term frequency）指的是某个词在文档中出现的频率，IDF（inverse document frequency）指的是如果包含某个词条t的文档n数目很少，那么IDF就越大，同时可以说明词条t具有很好的区分能力。

TFIDF的计算公式是TFIDF= TF*IDF，词频(TF)指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数的归一化，以防止它偏向长的文件。[copy],对于在某一个特定文件里的词语$t_i$来说，它的重要性可表示为$tf_i,_j = \frac{n_i,_j}{\sum_{k} n_k,_j}$，以上$n_i,_j$是该词在文件$d_j$中的出现次数，而分母则是在文件$d_j$中所有字词的出现次数之和。逆向文件频率(IDF)是一个词语普遍重要性的度量，可以表示为$idf_i = log\frac{\vert D \vert}{\vert \{j:t_i \in d_j \}\vert}$，其中$\vert D \vert:$ 是语料库中的文件总数。$\vert\{ j:t_i \in d_j\}\vert:$，包含词语$t_i$的文件数目。[copy]

基于TFIDF的关键词短语提取算法流程如下，

\begin{enumerate}
\item 将所有微博合并至一篇文档
\item 使用正则表达式提取文档中的名词短语
\item 计算名词短语的TFIDF值，进行排序
\item 提取top N的名词短语作为关键词短语
\end{enumerate}

TFIDF的优点是实现简单并且效果不错因而在实际中使用非常广泛，缺点就是TFIDF算法中并没有体现出单词的位置信息，因此存在改进的机会。

\section{基于LDA以及PageRank的关键词短语提取算法}

基于LDA以及PageRank的关键词短语提取算法基本流程是，首先基于微博构建词图(word graph)，后面会介绍词图的构建方法。其次基于LDA模型，给定n个主题，在词图上得到n个主题的概率分布以及每个单词在每一个主题下的概率值。接着基于词图,文档的主题分布,每一个主题下单词的概率值，在词图(word graph)上运行改进的PageRank算法。由于改进的PageRank算法可以利用文档的主题分布以及n个不同的主题下的单词分布，经过迭代收敛之后可以获取n个具有已经排好序的单词的词图。接着使用基于LDA以及PageRank的关键词短语提取算法提取关键词短语。
本章的组织如下，首先介绍算法中涉及到的LDA以及PageRank模型，然后介绍关键词短语提取算法。

\subsection{LDA模型介绍}

LDA是一种主题模型，一种无监督学习算法，是由Blei，Andrew Ng，Michael提出来的，它可以将文档的主题按照概率分布的形式给出。在LDA中，一篇文档是这样产生的，首先我们根据狄利克雷分布得到主题分布，从主题的多项式分布中得到文档第j个词的主题zj，然后我们从狄利克雷分布中取样生成主题zj的词语分布，最终我们根据这个词语分布得到某个单词。
LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。[1] 
LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。
LDA整体流程。。。。。。
先定义一些字母的含义：文档集合D，主题（topic)集合T
D中每个文档d看作一个单词序列<w1,w2,...,wn>，wi表示第i个单词，设d有n个单词。（LDA里面称之为wordbag，实际上每个单词的出现位置对LDA算法无影响）
·D中涉及的所有不同单词组成一个大集合VOCABULARY（简称VOC），LDA以文档集合D作为输入，希望训练出的两个结果向量（设聚成k个topic，VOC中共包含m个词）：
·对每个D中的文档d，对应到不同Topic的概率θd<pt1,...,ptk>，其中，pti表示d对应T中第i个topic的概率。计算方法是直观的，pti=nti/n，其中nti表示d中对应第i个topic的词的数目，n是d中所有词的总数。
·对每个T中的topict，生成不同单词的概率φt<pw1,...,pwm>，其中，pwi表示t生成VOC中第i个单词的概率。计算方法同样很直观，pwi=Nwi/N，其中Nwi表示对应到topict的VOC中第i个单词的数目，N表示所有对应到topict的单词总数。
LDA的核心公式如下：
p(w|d)=p(w|t)*p(t|d)
直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。
实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt

我们使用LDA期望得到文档的主题分布以及某个主题下面的单词分布。

\subsection{PAGERANK模型介绍}

PageRank算法是Google用来进行对网页进行排序的算法，PageRank算法是一种链接分析的算法，它将会给予超链接网页集合中的每一个网页一个数值权重，这个权重可以用来衡量此网页在超链接网页集合中的重要性。PageRank算法的做法首先将网页建模成图，图中的每个节点(网页)都有入度以及出度，每一个点的权重是由指向这些点来决定的，然后再加上一项是随机因子，保证每个节点有相同的概率挑往其他节点。经过迭代之后每一个节点可以得到稳定的权值。PageRank每个节点的入度是指向此节点的节点数量，每个节点的出度指的是当前节点指向的其它节点的数量。

PageRank算法输出一个概率分布，用来表示人们随机点击一个链接的概率。Pagerank算法的数学公式表示是，$ PR\left(p_i\right) = \frac{1-d}{N}+d\sum_{p_j \in M\left(p_i\right)} \frac{PR\left(p_j\right)}{L\left(p_j\right)}$$p_1,p_2,...,p_N$是被研究的页面，$M\left(p_i\right)$是链入$p_i$的页面的集合， $L\left(p_j\right)$是$p_j$链出页面的数量，$N$是所有页面的集合。

\subsection{关键词提取算法}

基于LDA和PageRank的关键词短语算法中使用的LDA模型以及PageRank模型已经在上面两节中介绍过了。下面我们阐述具体的算法。

首先在微博上运行LDA算法，得到微博上的n个主题分布以及每个主题下面的单词分布。我们已经在上面的章节中介绍过LDA模型。
其次基于微博构建词图，在词图上运行修改后的PageRank算法，原始的PageRank每个节点跳往其他节点是概率是相同的，这里做出的修改是，将这一项修改为单词(节点)在某个topic下面的概率分布，这样单词在跳转时候会倾向于跳转跟此topic相关的词语，我们使用$G=\left(V,E\right)$来表示有文档表示的图，

其中的节点集合$V=\{w_1,w_2,\dots,w_N\}$，边的集合$E$,$\left(w_i,w_j\right) \in E$如果$w_i$和$w_j$之间有一条链接，在词图中每一个节点表示单词，每一条边表示单词之间的边，接下来我们使用$e\left(w_i,w_j\right)$来表示链接$\left(w_i,w_j\right)$之间的权重，节点$w_i$的初度表示为$O\left(w_i\right) = \sum_{j:w_i \rightarrow w_j} e\left(w_i,w_j\right)$，在原始的PageRank中单词$w_i$的得分$R\left(w_i\right)$的数学公式是

$R\left( w_i \right) = \lambda \sum_{j:w_j \rightarrow w_i} \frac{e\left(w_j,w_i\right)}{O\left(w_j\right)}+\left(1-\lambda\right)\frac{1}{\vert V \vert}$，其中$\lambda$指的是衰减因子，取值范围0到1，$\vert V \vert$指的是节点的总数，衰减因子暗示每一个节点都有相同的概率$\left(1-\lambda\right)\frac{1}{\vert V \vert}$跳转到其它的节点。修改后的PageRank数学公式是$R_z\left( w_i \right) = \lambda \sum_{j:w_j \rightarrow w_i} \frac{e\left(w_j, w_i\right)}{O\left(w_j\right)}R_z\left(w_j\right)+\left( 1-\lambda \right)p_z\left(w_i\right)$。这里我们将$\left(1-\lambda\right)\frac{1}{\vert V \vert}$修改为$\left( 1-\lambda\right)p_z\left(w_i\right)$。这里跳转的概率不再是一致的，而是使用了单词在某一个主题下面的概率，这样节点在跳转时会倾向于跳转到同一个主题下面的单词。

在词图上运行改进的PageRank算法之后，可以得到每个主题下面的单词排序。基于关键词短语一般是名词短语这个假设，对经过分词之后的语料进行词性标注，词性标注指的是对于句子中的每个词都指派一个合适的词性，也就是要确定每个词是名词、动词、形容词或者其它词性的过程。[copy]接着使用正则表达式$\left(adjective\right)*\left(noun\right)+$抽取所有的名词短语，对于某一个名词短语首先计算一个主题下面的权重，使用的公式如下$R_z\left(p\right) = \sum_{w_i \in p} R_z\left(w_i\right)$，这里的$p$指的是名词短语，$w_i$指的是名词短语中的单词，$R_z\left(w_i\right)$指的是单词$w_i$在主题$z$上的经过排序之后的权重。考虑到文档中的n个主题分布，那么名词短语的最终权重需要考虑n个不同的主题分布，最终使用下面这个公式对所有的关键词短语排序，$R\left(p\right) = \sum_{z=1}^k R_z\left(p\right) \times pr\left(z \vert d \right)$。这里的$R_z\left(p\right)$指的是名词短语在一个主题下的权重，$pr\left(z\vert d\right)$指的是某个主题在文档的概率分布，$R\left(p\right)$指的是考虑了文档中的主题分布之后，名词短语最终的权重。

\section{关键词短语提取系统设计}

关键键词短语提取系统总共包括数据获取模块，关键词短语提取模块，前端展示模块。下面将简单的介绍
\begin{enumerate}
\item 数据获取模块
\begin{enumerate}
\item 包括微博登陆模块，爬取模块，存储模块
\item 使用python语言结合mongodb数据库进行开发
\end{enumerate}
\item 关键词短语提取模块
\begin{enumerate}
\item 包括数据预处理模块，topic mode模块，pagerank模块
\item 使用python语言结合gensim开发
\end{enumerate}
\item 前端展示模块
\begin{enumerate} 
\item 将关键词短语可视化的展示出来
\item 使用了tornado库结合bootstrap库以及D3.js库开发
\end{enumerate}
\end{enumerate}

本章接下来将依次简略介绍这3个模块。

\subsection{数据获取模块}
数据获取模块，主要包括微博登陆模块，爬取模块，存储模块，主要使用python语言结合MongoDB数据库进行开发。接下来将简单介绍每个模块功能，
\begin{enumerate}
\item 微博登录模块负责模拟登录微博，接下来就可以正常的爬取数据。
\item 爬取模块负责根据关键字进行微博爬取，将爬取数据到的数据存储到数据库。
\item 存储模块负责模拟存储爬取的微博，并且确保存储安全，效率高
\end{enumerate}

由于存储微博类似于存取文档，因此这里选择使用文档数据库MongoDB，MongoDB是一种文件导向数据库管理系统，MongoDB是一种非结构化数据库，也称为文档型数据库，MongoDB支持key-value的存储模式，这里key可以是我们的搜索关键字，value是基于关键词搜索出来的weibo。

\subsection{关键词短语提取模块}
关键词短语提取模块，主要包括数据预处理模块，关键词短语算法模块，主要使用python语言结合gensim开发。接下来将简单的介绍每个模块的功能，
\begin{enumerate}
\item 数据预处理模块负责清理数据，包括分词，去除停用词等。
\item 关键词短语模块负责使用相关算法计算名词短语权重，并且依据权重排序
\end{enumerate}
在关键词短语算法模块中使用了LDA模型，我们利用开源工具包Gensim中的LDA实现，Gemsim是Radim 힀eh킁힂ek开发的python包，里面有丰富的接口。对于Gensim的LDA的使用如下，
\begin{enumerate}
\item LDA训练使用方法,LDA train usage: Lda = ldaModel(corpus, num\_topics=100)
\item LDA预测使用方法，LDA inference usage 。。。。。。
\end{enumerate}

\subsection{前端展示模块}
前端展示模块需要将关键词短语可视化，这里使用了python的tornado库结合boostrap库以及D3.js库开发。前端开发使用的是Tornado以及boostrap库，可视化使用的D3.js库。下面将依次介绍。
Tornado是FriendFeed开发并且由Facebook开源的Web服务框架，它是非阻塞形式的服务器，而且速度相当快。
Bootstrap库由Twitter开发并且开源的前端开发框架，具有简洁，直观，强悍的特点，对于我们开发帮助很大。
D3.js是由stanford大学开发的开源工具，应用非常广泛，可视化操作简单，是主流的数据可视化工具。D3.js是一个基于数据的操作文档的JavaScript库，可以让你绑定任何数据到DOM，支持DIV这种图案生成，也支持SVG这种图案的生成，D3帮助使用者屏蔽了浏览器差异，因此可以做到代码简洁[cpoy]。


\section{实验设计及分析}
在关键词短语提取的实验中，我们尝试了2种方法，接下来依次介绍基于TFIDF关键词短语提取算法，基于LDA和PageRank方法的关键词短语提取算法的实验设计以及结果。

\subsection{基于TFIDF的关键词短语提取方法}
基于TFIDF的关键词短语提取算法流程，如下，
\begin{enumerate}
\item 新浪微博数据预处理，包括，
\begin{enumerate}
\item 去除重复的微博
\item 将所有的微博合并到一个文档里面
\item 使用分词工具进行微博分词
\item 去除停用词以及标点符号
\end{enumerate}
\item 使用正则表达式提取名词短语
\begin{enumerate}
\item 正则表达式是$\left(adjective\right)*\left(nouns\right)+$
\end{enumerate}
\item 对于名词短语根据TFIDF计算公式计算的权重
\end{enumerate} 

\subsection{基于LDA以及PageRank的关键词短语提取算法}
基于LDA以及PageRank的关键词短语提取算法流程，如下，
\begin{enumerate}
\item 新浪微博数据预处理,与TFIDF的方法一致
\item 使用处理后的微博构建词图,后面将详细介绍词图构建
\item 利用LDA算法获取文档的主题分布以及主题下面单词的概率分布
\item 使用改进后的PageRank算法计算每个主题下面的单词排序
\item 使用词性标注工具对文档进行词性标注
\item 使用正则表达式抽取名词短语，正则表达式跟TFIDF方法一致
\item 依据上文中的名词短语权重计算公式计算最终的权重
\end{enumerate}
在算法流程中提到了词图的构建，下面介绍怎么构建词图，

首先介绍窗口概念，窗口是以当前词作为源点，以接下里的若干个词作为终点，接着以"I Love Nanjing University very much"中的单词I为例介绍词图。假设窗口为1，那么可以得到边是["I->love"]，窗口为2，可以得到的边是["I->Nanjing"，"I->university"]，窗口为3，可以得到的边是["I->love"，"I->Nanjing"，"I-university"]。在构建基于微博的词图时候，需要将一定数量的微博放在同一个文本里，以此文本为单位构建词图。

\section{总结}

在本章节中，我们在新浪微博语料集上实现了基于LDA和PageRank的关键词短语提取算法，算法效果不错。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{domain classification using slot feature based SVM}\label{chapter_smallworld}

\section{研究背景}

    随着智能手机的大量使用，语音助手类的应用将会越来越多的出现在人们的实际生活中，对于语音助手的研究越来越普及。现在比较流行的语音助手有苹果公司开发的Siri，谷歌公司开发的Google Now，而现在微软也推出一款叫做Cortana的语音助手。Cortana基于人工智能以及大数据的技术，尝试去理解用户的喜好和习惯，具有帮助用户进行日程安排，问题回答等等功能。

    在Cortana的后台处理逻辑中包括一个称之为领域分类(domain classification)的模块。领域分类模块需要处理的输入有经过语音识别模块处理之后的短文本或者通过手写送过来的短信等短文本。领域分类模块负责对输入短文本的进行分类，也就是将输入的短文本分配到相应的类中去。举例来说，对于查询(query),"今天苏州下雨了吗"，领域分类模块需要将此查询分类到天气领域，接下来将有其它模块对于分类过的查询进行相应的处理。因此领域分类模块的准确率非常重要，这是接下来对于查询进行一系列更深入处理的入口。微软的语音助手Cortana总共有9个领域，分别是$alarm,places,reminder,calender,weather,note,date,news,web$。因此领域分类模块需要处理的是对于每一个未知的查询输入，将此未知的查询进行分类，分配到上面9个领域中对应的领域中去。

    在领域分类中我们的处理的任务是分类。在机器学习领域，分类被认为是一种有监督学习算法，是使用最普遍的一类算法。分类算法通过对已经标注过类别的语料集分析，通过提取训练语料集的特征建立分类函数，从中发现分类的规则，因而可以用来对未知的数据预测类别。在实际应用中，分类算法使用非常广泛比如金融机构中的风险评估，客户类别的分类，搜索引擎中的文本分类以及软件项目中的漏洞挖掘分类。在分类算法中，广泛使用的有最近邻模型(K Nearest Neighbor)，朴素贝叶斯模型(Naive Bayes)，支持向量机模型(Support Vector Machine)，神经网络模型(Neural Network)，决策树模型(Decision Tree)等等。分类算法的评价指标有准确率，预测速度，可扩展行以及可解释性等等。
    
    对于微软语音助手Cortana的领域分类处理模块来说，处理的输入是文本或者更确切的说是短文本，因此可以将Cortana中的领域分类任务当做是文本分类的一种。对于文本分类来讲，数据的特点是数据规模比较大，特征比较稀疏，支持向量机(SVM)算法经过几十年的使用实践，已经被证明可以在文本分类的领域中有效地使用。因此在领域分类任务中选择使用支持向量机算法。
    
    由于Cortana中的领域分类模块处理的输入查询长度非常短，因此对于每一个查询来讲，可以提取利用的特征数量比较少，基于此想到可以为查询扩充特征，借助这些扩充的特征来提高领域分类的准确率。实际上在微软的给定的语料中，每一条查询都被人工标上slot，slot是一种领域相关的实体，在后续章节中将会详细介绍slot概念。在领域分类任务中将会尝试对于标注了slot的查询处理，提取相应的特征进行分类，希望提取的结合了slot特征可以帮助提高领域分类的准确度。实验证明，对每一条标注了slot的查询进行分类确实可以帮助提高准确率。
    
    本章接下来的组织如下，首先我们将概述两种基于slot的分类算法以及两种算法中涉及到的条件随机场模型，支持向量机模型，然后依次详细介绍基于slot的分类算法以及实验设计和结果，最后总结两种方法的优劣。

    
\section{基于slot的分类算法介绍}
   
    首先介绍slot概念，slot是一种实体领域相关，在研究背景这一章节中，我们已经介绍了Cortana共有9个domain。下面以alarm以及weather 领域为例来介绍slot具体概念，在alarm领域slot中包括<alarm\_state>，<set\_alarm>，......，<start\_time>等等。在alarm领域已经标注了slot的query是"闹钟 设 在 <start\_date> 明 </start\_date> <start\_time> 早 2 点 </start\_time>"，在weather领域中slot包括<data\_range>，<weather\_condition>，......，<suitable\_for>等等。在weather领域中已经标注了slot的query是"今天 苏州 下雨 <weather\_condition> 了"

    我们使用了两种基于slot的领域分类算法,一种是基于generic slot tagger的domain classification方法，一种是基于in-domain slot tagger的domain classification方法。上面两种方法中的generic slot tagger以及in-domain slot tagger是一种使用条件随机场(CRF)模型训练之后得到的标注模型。
    
    所谓基于generic slot tagger的分类算法，首先对于所有的领域，使用条件随机场模型生成一个slot tagger，然后利用这个唯一的slot tagger对于每一个未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知的query进行分类。

    所谓基于in-domain slot tagger的分类算法，首先对于每一个领域，使用条件随机场模型生成一个slot tagger，然后对于每一个未知的query，分别使用9个in-domain slot tagger之一对未知的query标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知query进行分类。
    
    微软给定的语料集中每一条query已经标注好slot，为了验证slot特征是否可以提高领域分类的准确率，我们设计了基于人工标注slot的微软数据集的领域分类实验。通过这个实验我们得到了一个oracle(ground truth)结果，准确率是96.8\%。相比于baseline的准确率86.3\%结果，有超过10\%的绝对提升，这是非常显著的提高。通过这个实验说明了slot特征可以用于提高领域分类准确率。实际上，对于在线上出现的大量的未知查询，使用人工标注slot是不可行的无论是人工成本或者时间成本都是不可接受的。因此我们希望可以使用一种机器学习算法来帮助实现slot的自动标注，在数据序列化标注中经常用的机器学习算法有隐马尔科夫模型(HMM),条件随机场模型(CRF)以及最大熵马尔科夫模型(MEMM)。基于条件随机场模型在微软使用广泛成熟并且已经效果不错，我们选择条件随机场算法(CRF)来实现slot的标注。
    
    无论对于基于generic slot tagger分类算法或者基于in-domain slot tagger的分类算法来说，处理流程都是首先对数据标注slot，使用条件随机场模型,然后对标注之后的数据进行分类，使用支持向量机模型。因此在接下来的章节中，我们将依次介绍条件随机场模型以及支持向量机模型。

\subsection{条件随机场模型}

条件随机场(CRF)由Lafferty等人于2001年提出，结合了最大熵模型和隐马尔可夫模型的特点，是一种无向图模型，近年来在分词、词性标注和命名实体识别等序列标注任务中取得了很好的效果。条件随机场是一个典型的判别式模型，其联合概率可以写成若干势函数联乘的形式，其中最常用的是线性链条件随机场。若让x=(x1，x2，…xn)表示被观察的输入数据序列，y=(y1，y2，…yn)表示一个状态序列，在给定一个输入序列的情况下，线性链的CRF模型定义状态序列的联合条件概率为
p(y|x)=exp{} (2-14)
Z(x)={} (2-15)
其中:Z是以观察序列x为条件的概率归一化因子；fj(yi-1，yi，x，i)是一个任意的特征函数；是每个特征函数的权值。
我们可以将slot标注认为是序列化标注问题，因此这里采用CRF作为我们的序列化标注模型。
我们使用两种不同的slot生成方法，一种是generic slot tagger，这种方法是对于包含所有domain的语料集生成一个CRF model，然后利用这个CRF model标注未知的query。一种是in-domain的slot tagger，这种方法为每一个domain单独生成一个CRF model，因为Cortana中的domain数量有9个，所以我们有9个不同domain的CRF model。这样在遇到未知的query时候，9个不同domain的CRF model都要为这个query进行标注，最后我们得到的是经过9个经过标注过slot的query。
接下来我们将首先介绍结合了generic slot model的方法，然后介绍in-domain slot model，最后比较两种不同方法优劣。

\subsection{支持向量机模型}

    在SVM中引入核函数，具体说就是可以将非线性可分的数据扩展到更高维空间使得可以线性可分，这大大的扩展了SVM的能力，使得SVM应用更加广泛。然而对于数据规模大，特征数量多并且比较稀疏的语料来说，引入核函数固然可以取得不错的结果，但是却要付出很大的时间代价。有研究表名，在处理这种数据时，使用线性核的SVM可以取得相似的准确率，同时可以极大的减少训练时间。因此对于我们要解决的基于短文本的domain classification来说，可以使用结合linear kernel的SVM，处理时间非常快并且可以获得不错的准确率。在实验过程中我们选取台湾大学林智仁老师组开发的Liblinear作为我们的实验工具。LibLinear实现高效并且易于使用，可以极大的提高我们的实验效率，在这里我们要感谢Liblinear的开发者。
我们使用SVM算法作为我们的分类算法，SVM算法在工业界中使用广泛并且效果不错。在实际使用中，我们希望我们model训练时间足够短以及model足够小，可以减少存储空间的要求。由于我们的语料集非常大，特征数量多并且稀疏，所以我们考虑使用线性核的SVM，这样可以减少训练时间，同时为了减少生成的model的size。我们使用了L1 正则 L2 Loss的linear kernel SVM。
我们的baseline使用的特征是提取query的ngram来得到，具体来说我们使用的是unigram，bigram，trigram组成的特征。
Ngram是自然语言处理常用的语言模型。
我们的对比算法中使用了额外的slot特征，下面来解释下什么是slot，slot可以理解为一个实体，作为例子，我们以以下的query为例，“闹钟 设 在 明 早 2 点”，标注了slot之后的query为“闹铃 设 在 《start\_date》 明 《/start\_date》 《start\_time>  早 2 点 《/start\_time>”这里的<start\_date>, </start\_date>, <start\_time>, </start\_time>是slot。
在处理未知的query时候，使用人工为未知的query标注slot是不切实际的。那么我们如何得到slot。上面我们提到过我们语料中400w query，这400w query中已经有标注好的slot数据，我们希望学习一种可以对query进行序列化标注的模型，这种经过学习过的模型可以给未知的query标注slot。常用的序列化标注模型，有隐马尔科夫模型HMM，条件随机场模型CRF，基于我们已经有实现好的高效的CRF model，这里我们选择使用CRF model来学习slot的标注。基于此我们可以使用crf模型来为未知的query预测slot。
在我们的后续实验过程中，结合slot作为特征来进行分类，我们可以得到的分类精确度比baseline可以提高大概1.9个点。
下文中我们将支持向量机模型简称为SVM，将条件随机场模型简称为CRF。


SVM算法是一种经典的机器学习算法，是由Boster，Guyon，Vapnik在1992年提出来的，SVM算法的特点是可以同时最小化经验误差与最大化几何间隔。从几何意义上来讲SVM算法寻找一个超平面，这个超平面用来分离开数据点，因此超平面也被成为分离面。基于SVM的优良性质，SVM可以应用非常广泛，比如文本分类，蛋白质识别，手写数字识别等等。
SVM算法在文本分类中的应用非常广泛而且性能非常好。SVM可以描述为最大化几何间隔，SVM形式很多，具体到我们的问题，我们使用的L1正则L2损失的SVM，SVM算法优化公式，L1+L2。
由于使用了L1正则，我们可以得到稀疏模型，首先我们解释在SVM中使用正则项的目的是避免过拟合The feasible point that minimizes the loss is more likely to happen on the coordinates on graph (a) than on graph (b) since graph (a) is more angular.  This effect amplifies when your number of coefficients increases, i.e. from 2 to 200. The implication of this is that the L1 regularization gives you sparse estimates. Namely, in a high dimensional space, you got mostly zeros and a small number of non-zero coefficients. This is huge since it incorporates variable selection to the modeling problem. In addition, if you have to score a large sample with your model, you can have a lot of computational savings since you don't have to compute features(predictors) whose coefficient is 0
所以通过使用L1正则我们可以得到一个稀疏化的模型，也即我们可以得到一个比较小的model。
传统上SVM是用来解决二分类的，怎么讲SVM用于多分类呢，方法有如下集中，one-vs-rest，one-vs-one以及。。。。。。
假设我们有n个类，one-vs-one指的是我们从这n个类中选取两个类ni，nj出来，然后利用这两个类的数据训练出一个binary classifier。以此类推，我们将会有n\^2/2个分类器。当有测试数据时候，我们同时使用这么多分类器，然后得到n\^2/2个分数，选取分值最高所属的类作为测试数据的类。
假设我们有n个类，One-vs-rest指的是我们将我们将其中的一个类作为正例，其余n-1个类作为负例，训练出一个binary classifier。依次类推，最后我们将会得到n个binary classifier。当遇到一个测试数据时候，我们同时使用这n个binary classifier，这样我们将会得到n个分值，选取分值最高所属的类作为测试数据的类。
实际中我们使用的是one-vs-rest方法来做多分类。


\section{基于genetic slot tagger的分类模型}

    所谓基于generic slot tagger的分类算法，首先对于所有的领域，使用条件随机场模型生成一个slot tagger，然后利用这个唯一的slot tagger对于每一个未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知的query进行分类。
    
    本章节组织如下，首先将介绍基于generic slot tagger的分类算法的处理流程,然后是算法性能分析，接下来是实验设计及结果。

\subsection{处理流程}

    基于generic slot tagger的领域分类模型处理流程如下图所示，
    %\begin{figure}[htbp]
    %  \centering
    %  \includegraphics[width=0.5\textwidth]{}
    %  \caption{}\label{fig:test1}
    %\end{figure}
    
    这种流程处理的好处是对于领域分类任务来说可以灵活的加入更丰富的特征，结构化的信息可以容易的传递给二分类器。

\subsection{算法分析}

    从微软给定的语料集来看，Cortana有9个领域，每个领域有大约10种领域相关的slot。所谓领域相关，即是每个领域标注的slot种类不同，举例说明如下，对于alarm领域，slot种类为<alarm\_date>，<set\_alarm>，......，<start\_time>等等。对于weather领域，slot种类为<date\_range>，<weather\_condition>，......，<suitable\_for>。其它领域类似，9个领域slot种类总共是86个。在研究背景这一章节中的实验表明在使用了人工标注slot的微软数据集的分类任务上可以得到的ground truth的准确率为96.8\%。在线上实际运行时，不仅需要考虑分类算法的准确率而且要考虑分类算法的预测时间。接下来将会统计没有使用slot的分类算法的处理时间以及基于generic slot tagger的分类算法的处理时间。

\begin{enumerate}
 \item 假设没有使用slot的分类算法的处理时间为$t_1$，它的处理逻辑是直接使用支持向量机算法对标注过的未知查询进行分类，假设使用时间是$t_{svm}$，那么$t_1 = t_{svm}$。
 \item 假设基于generic slot tagger的分类算法的处理时间为$t_2$，它的处理逻辑是首先使用generic slot tagger对未知的查询标注slot，假设使用时间是$t_{crf}$，然后使用支持向量机算法对标注过slot的未知查询进行分类，假设使用时间是$t_{svm}$。总的运行时间定义为$t_2 = t_{crf} + t_{svm}$。
\end{enumerate}

    相比于没有使用slot的分类算法，基于generic slot tagger的分类算法时间上将会多出使用条件随机场模型对未知查询标注slot的时间$t_crf$。如果$t_crf$小于$t_svm$，那么我们是可以接受的，然而使用条件随机场模型对未知查询标注slot时，解码的时间复杂度是$O\left(T\times\vert S \vert^2\right)$，这里的s指的是slot种类数量，上面提到过Cortana总共有86种slot。使用支持向量机模型对未知查询预测分类的时间复杂度是$O\left(n*m\right)$。基于此使用条件随机场模型对未知查询标注slot时间远大于使用支持向量机模型对未知查询预测的时间，相比于之前没有使用generic slot tagger时候，时间增长太高。即使在上面的实验中使用人工标注slot的微软数据集的分类算法可以得到的ground truth的准确率为96.8\%,从时间上考虑，对于线上应用也是不能接受的。

    为了满足线上时间需求，我们想到的是减少slot种类，从而可以减少generic slot tagger的解码时间。我们的方法是从86种slot中选取少量具有代表性的slot。经过讨论，选取了其中19种slot。接下来需要验证这19种slot是否可以提高领域分类的准确率。基于此，我们设计了下面的实验，对于语料集做了相应处理，即是对于所有标注slot的查询，只保留19种slot，其余的全部移除。然后在处理后的语料集中进行分类算法的实验，可以得到的ground truth准确率是91\%，相对于baseline的准确率86.3\%来讲这也是一个非常显著的提升。
    
    接下来关注基于微软语料集训练的条件随机场模型在实际中使用的效果。为了了解条件随机场模型的F值也即slot标注效果对于领域分类的影响。我们做了一个在微软语料集上模拟slot标注的实验，模拟序列化标注的行为，我们随机的对于每一条查询的slot进行增加，删除，改变三个行为。具体的来说就是对于每一个查询的每个词，当此词没有slot时，以一定的概率给定一个slot，当此词有slot时，以一定的概率选择删除slot或者使用其它slot替换此slot，经过如此处理之后计算模拟标注slot的F值。基于模拟实验，可以生成虚拟的F值范围为40\%-60\%之间标注模型。最后在处理过的语料集上进行分类算法实验，观察模拟实验得到的标注模型的不同的F值与准确率的关系，下图是我们的模拟实验。
    %\begin{figure}[htbp]
      %\centering
      %\includegraphics[width=0.5\textwidth]{}
      %\caption{}\label{fig:test1}
    %\end{figure}

    
    从图中曲线可以观察得知F值在60\%及以上，分类算法的准确率有提高。这意味着如果可以训练得到一个F值在60\%及以上的条件随机场模型，分类模型的准确率可以得到提高。接下来是我们的实验设计。

\subsection{实验设计}

    我们使用支持向量机模型来做领域分类任务。使用微软给定的语料集来实验，其中有400万查询作为训练数据，1.8万查询作为测试数据。我们使用LIBLINEAR来训练支持向量机模型，LIBLINEAR是台湾大学林智仁组开发的一种线性分类器，可以用来处理具有数以百万计实例和特征的数据。LIBLINEAR支持L2正则L2损失的线性SVM分类器，L2正则L1损失的线性SVM分类器，L1正则L2损失的线性SVM分类器等等。LIBLINEAR跟LIBSVM具有很多相似地方，比如跟LIBSVM处理数据格式一致，使用方法类似。对于多分类算法，LIBLINEAR支持使用one-vs-rest，Crammer\&Singer等等策略。同时LIBLINEAR接口丰富，支持MATLAB/Octave，Java，Python以及Ruby等等。LIBLINEAR使用分为训练过程以及预测过程如下，
\begin{enumerate}
    \item 训练过程的使用方式Usage: train [options] training\_set\_file [model\_file]
     \begin{enumerate}
       \item train是LIBLINEAR提供的训练工具
       \item training\_set指的是训练语料集
       \item model\_file指的是训练之后得到的模型文件
     \end{enumerate}
    \item 预测过程的使用方式Usage: predict [options] test\_file model\_file output\_file
      \begin{enumerate}
        \item predict是LIBLINEAR提供的预测工具
	   \item test\_file指的是测试语料集
	   \item model\_file是在训练过程中产生的模型，用于在测试语料集上预测
	   \item output\_file是trai测试语料test\_file上的预测输出
      \end{enumerate}
\end{enumerate}
    
     为了防止随机性对于实验结果的影响，我们重复实验50次，取准确度的平均值作为最终的准确率。实验设计流程如下，
\begin{enumerate}
  \item 对于给定语料进行数据预处理，处理过程如下，
    \begin{enumerate}
      \item 使用微软内部分词工具分词
      \item 去除停用词
    \end{enumerate}
  \item 特征提取，处理过程如下，
    \begin{enumerate}
      \item 特征集合是在训练集上提取
      \item 对于测试集来讲，所有不在特征集合的特征都被舍弃掉
      \item 在微软给定的语料集上得到的特征数量是875万
    \end{enumerate}
  \item 对每一条查询的提取特征进行归一化,归一化的好处如下，
    \begin{enumerate}
      \item 可以防止某一维或某几维对数据影响过大
      \item 可以使程序可以运行更快
    \end{enumerate}
  \item 将特征处理成支持向量机使用的格式
  \item 在处理好的训练集中使用LIBLINEAR训练支持向量机模型。
    \begin{enumerate}
      \item 在训练集上训练支持向量机模型
      \item 在测试集上测试支持向量机模型，记录模型的准确率
    \end{enumerate}
\end{enumerate}
    
    基于以上实验流程，我们分别实现了baseline模型以及基于generic slot tagger的模型。这两种模型的区别仅仅是使用的特征不同，对于baseline模型来讲，我们仅仅使用了文本特征，对于基于generic slot tagger的模型来讲，我们将query中的slot看成跟词一样的项，在特征提取过程中，提取了三种类型的特征。在特征提取的策略上，对于两种模型都是提取了ngram的特征，不同的地方是基于generic slot tagger的模型利用了slot实体。我们以查询"闹铃 设 在 <start\_date> 明 </start\_date> <start\_time> 早 2 点 </start\_time>"为例，讲述两种模型的特征提取方法。
    对于baseline模型的特征提取，我们提取了文本的unigram，bigram以及trigram特征，具体如下，
    \begin{enumerate}
      \item unigram特征[“闹铃”，“设”，“在”，“明”，“早”，“2”，“点”]
      \item bigram特征[“闹铃 设”，“设 在”，“在 明”，“明 早”，“早 2”，“2 点”]
      \item trigram特征[“闹铃 设 在”，“设 在 明”，“在 明 早”，“明 早 2”，“早 2 点”]
    \end{enumerate}

   
    对于基于generic slot tagger模型的特征提取，首先提取查询本身的ngram特征，其次提取slot集合的ngram特征，最后提取查询跟slot一起的ngram特征。具体如下表示。
\begin{enumerate}
  \item 基于查询本身的特征
    \begin{enumerate}
      \item unigram特征是[“闹铃”，“设”，“在”，“明”，“早”，“2”，“点”]
      \item bigram特征是[“闹铃 设”，“设 在”，“在 明”，“明 早”，“早 2”，“2点”]
      \item trigram特征是[“闹铃 设 在”，“设 在 明”，“在 明 早”，“明 早 2”，“早 2 点”]
    \end{enumerate}
  \item 基于slot本身的特征
    
   基于上述query提取的slot集合是"<start\_date> </start\_date> <start\_time> </start\_time>",对此slot集合提取unigram，bigram，trigram特征
    \begin{enumerate}
      \item unigram特征是["<start\_date>", "</start\_date>","<start\_time>", "</start\_time>"]
      \item bigram特征是["<start\_date> </start\_date>","</start\_date> <start\_time>", "<start\_time> </start\_time>"]
      \item trigram特征是["<start\_date> </start\_date> <start\_time>", "</start\_date> <start\_time> </start\_time>"]
    \end{enumerate} 
  \item 基于查询以及slot的特征
   
   在上述的查询中，我们将slot跟单词同样对待，对此查询提取unigram，bigram，trigram特征。
   \begin{enumerate}
     \item unigram特征是["闹铃"，"设"，"在"，"<start\_date>"，"明"，"</start\_date>"，"<start\_time>"，"早"，"2"，"点"，"</start\_time>"]
     \item bigram特征是["闹铃 设"，"设 在"，"在 <start\_date>"，"<start\_date> 明"，"明 </start\_date>"，"<start\_date> <start\_time>",],
     \item trigram特征是[]
   \end{enumerate}
\end{enumerate}
\section{实验结果分析}

    对于baseline模型，我们取得了86.3\%的准确率。
    
    对于基于generic slot tagger的分类模型，使用模拟的F值为60\%-80\%的模型进行标注时候，我们得到的标注模型的F值跟分类算法的准确率之间的关系如下图所示，

    %\begin{figure}[htbp]
    %  \centering
    %  \includegraphics[width=0.5\textwidth]{}
    %  \caption{}\label{fig:test1}
   % \end{figure}
    
    然而实际上在使用F值为60\%至80\% CRF模型对query进行标注，然后在标注之后上的语料进行分类算法的实验时得到的分类的准确率是86.1\%,并没有相应提高。这是一个需要未来解决的问题。由于基于generic slot tagger的分类模型没有取得预期的提高，接下来我们使用了基于in-domain slot tagger的分类模型，并且取得了准确率上的提高,我们将在下一章中进行介绍。

\section{基于in-domain slot tagger的分类模型}

    在上一章介绍过，在微软给定的语料集中，总共有9个领域，每个领域大约有10种slot。每个领域的slot种类跟其它领域之间几乎没有重合的，也即slot是领域相关的。

    所谓基于in-domain slot tagger的分类算法，首先对于每一个领域，使用条件随机场模型生成一个slot tagger，然后对于每一个未知的查询，分别使用9个in-domain slot tagger之一对未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知查询进行分类。
    
    本章节组织如下，首先将介绍基于in-domain slot tagger的分类算法的处理流程,然后是算法性能分析，接下来是实验设计及结果。

\subsection{处理流程}

    基于in-domain slot tagger的分类算法模型处理流程如下图所示，
    %\begin{figure}[htbp]
    %  \centering
    %  \includegraphics[width=0.5\textwidth]{}
    %  \caption{}\label{fig:test1}
    %\end{figure}
    
    这种模型跟基于generic slot tagger的模型具有相同的好处是对于分类任务来说可以灵活的加入更丰富的特征，结构化的信息可以容易的传递给基础的二分类器。

\subsection{算法介绍分析}
    
    在上面的介绍中，我们了解到Cortana有9个领域。在接下来的算法描述中，涉及到in-domain以及anti-domain概念，in-domain指的是9个领域中的任意一个领域，anti-domain指的是除了in-domain之外的其余所有领域。举例说明，假设alarm是in-domain，那么anti-domain指的是reminder，calender，palces，weather，web，date，news，note。那么in-domain slot tagger指的使用任意一个领域语料集训练出来的条件随机场模型。接下来介绍基于in-domain slot tagger的分类算法，分为训练过程以及预测过程具体如下，

   基于in-domain slot tagger的分类算法的训练过程，
   \begin{enumerate}
     \item 使用in-domain slot tagger对anti-domain的查询标注slot
       \begin{enumerate}
         \item in-domain语料集作为正例
         \item 生成的anti-domain语料集作为负例
       \end{enumerate}
     \item 对于in-domain使用支持向量机模型训练生成一个二分类器
     \item 基于Cortana的9个领域，训练出9个二分类器
   \end{enumerate}
  
   基于in-domain slot tagger的分类算法的测试过程，
   \begin{enumerate}
     \item 使用每一个in-domain slot tagger对未知的查询标注slot
     \item 使用每一个in-domain的二分类器对标注slot的查询预测打分
     \item 比较所有的打分，将查询的类设置为分数最高的in-domain
   \end{enumerate}
    
    与基于generic slot tagger的分类算法一样，需要考虑线上运行时间，接下来将会统计没有使用slot的分类算法的处理时间以及基于in-domain slot tagger的分类算法的处理时间，具体如下：
\begin{enumerate}
 \item 假设没有使用slot的分类算法的处理时间为$t_1$，它的处理逻辑是直接使用SVM算法对标注过的未知查询进行分类，假设使用时间是$t_{svm}$，那么$t_1 = t_{svm}$。
 \item 假设基于in-domain slot tagger的分类算法的处理时间为$t_2$，它的处理逻辑是首先使用每一个in-domain的slot tagger对未知的查询标注slot，对于9个领域来讲这个过程是可以并行执行的，假设使用时间是$t_crf=\max\limits_{i \in I} t_{{crf}_i},I \in (0,9)$ ，然后使用支持向量机算法对标注过slot的未知未知进行分类，假设使用时间是$t_{svm}$。总的运行时间定义为$t2 = t_{crf} + t_{svm}$。
\end{enumerate}

    因为每个domain中slot种类数量不多，每个domain不仅可以容易的获取F值很高的条件随机场模型。同时对于每个domain的条件随机场模型解码时间在可以接受的范围内。
    
    接下来我们将会做一组实验，对比baseline分类模型与基于in-domain slot tagger的分类模型准确率。

\subsection{实验设计以及结果}
    
    基于in-domain slot tagger的分类算法与基于generic slot tagger的分类算法数据处理，特征提取等方式是一致的。不同在于，每个领域的条件随机厂模型的生成以及对于未知查询的标注过程。接下来我们将简述不同之处。
   
    如何训练in-domain slot tagger
    \begin{enumerate}
      \item 对于每个领域首先随机选取3000个sentences作为训练数据，1000个句子作为测试数据。
      \item 使用微软内部训练工具，对于每个domain训练出in-domain slot tagger。
      \item 经过仔细微调之后，每个domain的slot tagger F值大于90\%
    \end{enumerate}

    当面对未知的查询时候，使用每一个domain的slot tagger对此查询标注slot。

    对于baseline模型，我们取得了86.3\%的准确率。对于基于in-domain slot tagger的分类模型,我们取得了88.2\%的准确率。
    综上，基于in-domain slot tagger的分类模型在准确率上得到了提高同时运行时间在可以接受的范围内，所以这个算法可以在线上运行。

\section{总结}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{incremental learning using SVM}\label{chapter_smallworld}

\section{研究背景}

   随着人工智能技术以及大数据技术的迅速发展，语音助手上的技术越来越成熟。同时随着智能手机的普及，使用智能手机上的语音助手的人越来越多。因此从语音助手中可以获取的数据量越来越大。这对于公司的信息挖掘以及知识获取是一种挑战，随着信息的更新速度越来越惊人，公司可以获取的数据量将会越来越丰富。
   
   对于微软的语音助手Cortana来说,每周都可以从Cortana的对话记录中得到越来越多的数据。在上一章中我们提到了使用基于Cortana对话语料集来实现查询分类的任务。其中使用支持向量机(SVM)算法用来分类，对于机器学习算法来说，泛化能力可以用来衡量机器学习算法的效果。所谓泛化能力指的是机器学习算法对于未知数据的预测能力，具体到查询分类中使用的支持向量机(SVM)算法，泛化能力指的是使用支持向量机(SVM)算法训练出来的模型对于未知查询预测效果。基于此，我们希望经过支持向量机训练出来的分类模型具有更强的泛化能力。
   
   模型的泛化能力受两方面的影响，一方面是模型的复杂度，泛化能力跟模型的复杂度成反比，一方面是数据的规模，泛化能力跟数据规模成正比。大规模的数据对于提升模型的泛化能力很重要。这意味着如果我们可以将Cortana更新之后的数据全部利用起来，那么就可以帮助提升模型的泛化能力，具体到查询分类来讲就是可以提升使用支持向量机训练的分类模型对于未知查询的分类准确度。
   
   利用大规模的数据训练算法模型的同时也伴随着另一个不能忽视的问题，随着可以利用的数据越来越多，在模型的训练时间上付出的代价越来越显著。对于分类任务来说即将面临的问题是一方面大规模的数据可以帮助训练出泛化能力更强的模型，另一方面如果每次都是重新开始训练模型，训练时间将会大大的增加，这对于在线上使用是不可以接受的。实际上，对于分类任务来讲，一方面我们已经在原始语料集上训练出了效果很好的模型，另一方面每周都会有大规模数据到来。为了提高分类模型的精确度，我们可以选择使用更新后的语料集来重新训练支持向量机(SVM)模型，这将伴随着一定的时间代价。同时我们希望可以利用一种算法，可以取得跟重新开始训练的模型一致的准确度并且大大的减少训练时间，至少可以满足现在的训练时间上的需求。台湾大学林智仁组发表了paper$"Incremental and decremental training for linear classification"$,提出了一种incremental learning的算法。基本思想可以概述为训练新的分类模型时，一方面使用更新后的数据进行训练，另一方面利用之前使用原始数据训练出来的模型参数初始化新模型的参数。那么新的模型既可以取得跟使用更新之后的数据重新训练的模型一致的准确度又可以减少训练时间。我们期望可以在微软给定的数据集中应用这个算法，并且在微软的给定的语料集上重现出实验效果。

   本章接下来的组织如下，首先我们将概述基于支持向量机(SVM)的incremental learning算法，然后介绍incremental learning算法求解中涉及到的坐标梯度下降算法以及置信域的牛顿算法，接着阐述实验设计以及结果，最后进行总结。

   在接下来的文章中，我们将支持向量机称作SVM，将增量学习算法称作incremental learning。

\section{基于SVM的incremental learning算法}

    我们求解的是基于SVM的incremental learning算法，在第二个章我们已经详细介绍过SVM，由于SVM的优化公式的形式属于凸优化范畴，对于凸函数来说存在着全局最优解。这意味着可以使用很多高效的优化算法来求解SVM算法的参数。需要特别指出的是incremental learning算法中使用的SVM是L2正则L2损失函数形式。SVM的求解算法可以分为基于原始问题的求解算法以及基于对偶问题的求解算法。

    接下来将直接介绍SVM原始问题的形式以及求解方法，对偶问题的形式以及求解方法以及这两种问题的求解方法与incremental learning的联系。

\subsection{SVM原始问题介绍}

    给定训练集$\left(y_i,x_i\right) \in \{-1, 1\} \times R^n, i = 1,\dots,l$,SVM原始问题的优化公式是$\min\limits_{w} f\left(w\right) where f\left(w\right)\equiv \frac{1}{2}w^Tw + C\sum{i=1}^l \xi\left(w;x_i;y_i\right)$, $\xi\left(w;x_i,y_i\right)$是损失函数，这里使用的是L2损失函数，也即$\xi_L2\left(w;x_i,y_i\right) \equiv \max\left(0,1-y_iw^Tx_i\right)^2$。L2损失函数的特点是可微但不是二次可微。存在很多凸优化算法寻找最有的参数$w$。

   在原始问题中，SVM的参数w的维度跟数据变化无关，跟特征数量一致。当训练数据增加时候，对于incremental learning算法有以下两个假设，第一个假设是，特征数量没有发生变化，因此对于原始问题需要求解的参数数量也没有发生变化。第二个假设是，新增加的数据跟原始数据服从于同一分布。因为求解原始问题的优化算法是一种迭代算法，因此好的初始值可以减少迭代次数从而减少训练时间。对于原始的训练集$\left(y_i,x_i\right),i=1,\dots,l$，假设$w^*$是原始问题的最优解，基于以上假设当增加新的数据时，可以利用$w^*$来作为新的模型参数的初始值也即$\bar{w}=w^*$。

    新数据增加后的优化公式是$\min\limits_{w} \frac{1}{2}w^Tw+C\sum_{i=1}^{l+k} \xi\left(w;x_i,y_i\right)$，如果使用了原始数据训练出来的模型参数$w^\ast$作为新的优化问题的初始化参数，那么新的问题的初始值是$\frac{1}{2}\bar{w}^T\bar{w}+C\sum_{i=1}^l \xi\left(\bar{w};x_i,y_i\right)+C\sum_{i=l+1}^{l+k}\xi\left(\bar{w};x_i,y_i\right)$。

\subsection{SVM对偶问题介绍}
    
    给定训练集$\left(y_i,x_i\right) \in \{-1,1\} \times R^n$，最优化参数$w$可以表示为带有系数$\alpha$的训练样本的线性组合，也即$w=\sum_{i=1}^l y_i\alpha_ix_i$。我们可以解决基于$\alpha$的优化问题，也就是SVM问题的对偶形式,$\max\limits_{\alpha} f^D\left(\alpha\right) \equiv \sum_{i=1}^l h\left(\alpha_i,C\right)-\frac{1}{2}\sum_{i=1}^l \sum_{j=1}^l \alpha_i\alpha_j K\left(i,j\right)-\sum_{i=1}^l \frac{\alpha_i^2}{2}d$,$0 \le \alpha_i \le U$,$\forall i=1,\dots,l$，这里的$K\left(i,j\right)=y_iy_jx^T_ix_j$,同时对于L2损失的SVM来讲有，$U=\infty$,$d=\frac{1}{2C}$,$h\left(\alpha_i,C\right)=\alpha_i$。
    
    对于对偶问题，求解的参数是$\alpha$,$\alpha$的维度跟数据维度一致，因此当增加新的数据时，$\alpha$的数量会发生变化。这里假设增加的新的训练集如下$(y_i,x_i),i = l+1,\dots,l+k$,下面的$\bar{\alpha}$可以视为初始的解，$\bar{\alpha} = [\alpha^\ast_1,\dots,\alpha^\ast_l,0,\dots,0]^\intercal \in R^{\left(l+k\right) \times l}$对于$\bar{\alpha_{l+1}},\dots,\bar{\alpha_{l+k}}$可以在$[0,U]$之间任意取值，但是不能确定如何最好的赋值。这也是求解incremtal learning中的对偶问题面临的问题。

    新数据进来后优化公式是$\min\limits_{w} \frac{1}{2}w^\intercal w+C\sum_{i=1}^{l+k} \xi\left(w;x_i,y_i\right)$，假设我们对$\bar{\alpha_{l+1}},\dots,\bar{\alpha{l+k}}$取值为0，那么增加新的数据之后优化公式的初始值是$\sum_{i=1}^l h\left(\alpha^\ast_i,C\right)+\sum_{i=l+1}^{l+k} h\left(0,C\right)-\frac{1}{2}[{\alpha^\ast}^\intercal O^\intercal]\left[{\begin{array}{ccccc} \bar{Q} & \vdots \\ \dots & \\ \end{array}}\right] = \frac{1}{2}\bar{w}^\intercal \bar{w}+C\sum{i=1}^l \xi\left(\bar{w};x_i,y_i\right)$
    
    在paper$Incremental and decremental learning for linear classification$中有证明，原始问题初始值相对于对偶问题初始值距离新的问题的最优值更近。同时对偶问题的初始值不好确定，综上，在求解基于SVM的incremetal learning问题时候选择求解svm原始问题。

\section{incremental learning中的优化算法}

优化算法是一种应用数学方法，在机器学习中使用非常广泛，下面我们简单介绍最优化方法形式，以及一般的求解方法，最优化方法主要研究以下形式的问题：给定一个函数$f:A\rightarrow\Re$，寻找一个元素$X^0 \in A$，使得对于所有$A$中的$x$，$f\left(x^0\right) \le f\left(x\right)$(最小化);或者$f\left(x^0\right) \ge f\left(x\right)$(最大化)。很多优化问题都可以建模成这样的一般性形式。一般情况下，会存在若干个局部的极小值或者极大值。局部极小值$x^\ast$定义为对于一些$\delta>0$，以及所有的$x$满足$\Vert x-x^\ast\Vert$,公式$f\left(x^\ast \right) \le f\left(x\right)$。成立。这就是说，在$X^\ast$周围的一些闭球上，所有的函数值都大于或者等于在该点的函数值。一般的，求局部极小值是容易的，但是要确保其为全域性的最小值，则需要一些附加性的条件，例如，该函数必须是凸函数。
    
    对于优化算法，经常使用的有两种，一种是一阶优化算法，这种算法的优点是每次迭代代价比较小，具体来说就是求解一阶导数的代价较低，缺点是迭代次数比较多。另一种是二阶优化算法，这种算法的优点是迭代次数比较少，因为这里求解的是二阶导数，是梯度的梯度，缺点每次迭代的代价比较大，具体来说要求Hessian矩阵的逆矩阵。
    
    由于需要求解的优化公式具有非常好的性质，具体的说求解的优化函数是一种凸函数，具有全局最优解，尤其需要注意的是求解的SVM是L2正则L2Loss函数，可以容易的求得优化函数的一阶导数以及二阶导数，因此可以使用一系列高效的优化算法。这里选择两种求解SVM问题比较高效的优化算法，通过比较两种优化算法在求解优化函数的时间同时关注两种优化算法求解的SVM模型在测试集上的准确率，最后确定一种比较好的的优化算法。这两种优化算法是$Coordinate Descent Method for Large-scale L2 loss Linear SVM$中提到的Coordinate Descent Method优化算法，$Trust Region Newton Method for Large-scale Logistic Regression$提高的trust region newton method优化算法。

\subsection{坐标梯度优化算法}

在$Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines$中提到的坐标梯度优化算法可以用来解决SVM的原始问题，并且相当高效。Incremental learning算法中求解是L2正则L2损失的SVM形式如下，$\min\limits_{w} f\left(w\right) = \frac{1}{2}w^\intercal w+ C\sum_{j=1}^l \max\left(1-y_jw^\intercal x_j,0\right)^2$,L2损失SVM是分段二次凸函数，一阶可微但不是二阶可微的。坐标梯度优化算法是一种迭代算法，通过每一轮对参数$w$进行迭代计算，产生序列$\{w^k\}_{k=0}^{\infty}$，直至参数收敛。

坐标梯度算法每次通过解决子问题来更新参数w的一个变量，如果可以高效的解决这个子问题那么坐标梯度算法将会很有竞争力。对于L2损失的SVM，子问题是最小化单变量的分段二次函数，这个函数一阶可微但是二阶不可微。形式化表示为$w_i \leftarrow w_i + \arg\min_{d} f\left(w+de_i\right)$。坐标梯度下降算法中使用了牛顿方法来近似的求解上述子问题并且具有线性的收敛速度[5]


\subsection{置信域牛顿优化算法}
   
    置信域牛顿优化算法解决的SVM问题与坐标梯度优化算法一致。对于牛顿方法来说，每一轮迭代的代价高同时具有很快的收敛速度。

    在每一轮迭代中对于参数w，TRON获得近似的牛顿步长d来获取置信域通过解决下面的子问题,$\min_{d} q\left(d\right) subject to \Vert d \Vert \le \Delta， where q\left(d\right) \equivt \Delta f\left(w\right)^\intercal+\frac{1}{2} d^\intercal \Delta^2 f\left(w\right)d$。这个子问题是通过共轭梯度下降进行求解的。


\section{实验设计以及结果分析}

    实验使用微软给定的语料集，其中400万条查询作为训练语料集，1.8万条查询作为测试语料集。为了防止随机性对于实验结果的影响重复实验50次，取平均的训练时间以及准确率作为做最终的结果。

    每次实验中，训练集按照如下方法获取，首先将训练语料集进行洗牌处理，然后将数据划分为10份，也即第一份数据包含原始数据的$\frac{1}{10}$,第二份数据包含原始数据的$\frac{2}{10}$,$\dots$,最后一份数据包含$\frac{10}{10}$。每次实验有十份不同的训练集，测试集依然是原始的测试语料集。这里称每次都是重新训练的方法为non-incremental训练方法，每次训练使用以前模型参数初始化的方法为incremental训练方法。
    
    接下来将分别阐述non-incrementallearning训练方法和incremental learning训练方法以及具体的实验设计。

\subsection{non-incremental learning训练方法}

对于non-incremental learning训练方法，分别使用原始数据经过洗牌处理之后的$\frac{1}{10}，\frac{2}{10}，\frac{3}{10}，\dots,\frac{10}{10}$的数据作为训练集，训练出分类器$X_1,X_2,X_3,\dots,X_10$。以第一个，第二个以及第十个分类器为例说明训练方法。
    
    \begin{enumerate}
	    \item 第一个分类器，使用$\frac{1}{10}$数据作为训练集，重新开始训练出分类器$X_1$。记录$X_1$的训练时间以及在测试集上的准确率。
	    \item 第二个分类器，使用$\frac{2}{10}$数据作为训练集，重新开始训练出分类器$X_2$。记录$X_2$的训练时间以及在测试集上的准确率。
      \item 第十个分类器，使用完整的语料集作为训练集，重新开始训练出分类器$X_10$。记录$X_10$的训练时间以及在测试集上的准确率。
    \end{enumerate}

\subsection{incremental learning训练方法}
    
对于incremental learning方法，分别使用原始数据洗牌之后的$\frac{1}{10},\frac{2}{10},\frac{3}{10},\dots,\frac{10}{10}$的数据作为训练集训练分类器，不同的是训练方法。以第一个，第二个，第十个分类器为例说明训练方法。

   \begin{enumerate}
	   \item 第一个分类器，使用$\frac{1}{10}$的数据作为训练集，训练出分类器$Y_1$。记录$Y_1$在的训练时间以及在测试集上的准确率
	   \item 第二个分类器，使用$\frac{2}{10}$的数据作为训练集同时使用分类器$Y_1$的参数作为第二个分类器参数的初始值，训练出分类器$Y_2$。记录$Y_2$的训练时间以及在测试集上的准确率
        \item 第十个分类器，使用完整的语料集作为训练集同时使用分类器$Y_9$的参数作为第十个分类器参数的初始值，训练出分类器$Y_10$。记录$Y_10$的训练时间以及在测试集上的准确率
    \end{enumerate}
\subsection{实验设计流程}
    
    对于non-incremental learning方法和incremental learning方法中的每一个分类器，对于其中的训练集以及测试集的处理方法一致，具体如下，

    \begin{enumerate}
      \item 训练集以及测试集进行数据预处理，具体如下
	\begin{enumerate}
	  \item 使用微软提供的分词工具分词
	  \item 去除停用词
	\end{enumerate}
     \item 提取文本特征，特征提取方法跟上一章节实验中方法一致
     \item 对于提取到的文本特征进行归一化
       \begin{enumerate}
         \item 可以防止某一维或某几维对数据影响过大
         \item 可以使程序可以运行更快，减少训练时间
	 \item 将特征处理成LIBLINEAR数据处理格式
       \end{enumerate}
     \item 使用LIBLINEAR在训练集上训练出SVM模型，在测试集上测试模型的准确率
     \item 记录使用LIBLINEAR训练出SVM模型的时间以及在测试集上的准确率
   \end{enumerate}

\subsection{实验结果分析}

    实验结果如下图所示：
    %\begin{figure}[htbp]
    %  \centering
    %  \includegraphics[width=0.5\textwidth]{}
    %  \caption{}\label{fig:test1}
    %\end{figure}

    对于两种不同的训练方法，我们分别训练出10个分类器，同时记录了每一个分类器的训练时间以及准确率。然后我们比较了两种不同训练方法得到的对应的每个分类器的训练时间以及准确率。从准确率的角度上看，non-incremental learning以及incremental learning方法取得准确率类似，误差在。。。，从训练时间的角度看，incremental learning方法的训练时间远远小于non-incremental learning方法。基于以上实验结果，可以证实incremental learning方法可以在线上使用。

\section{总结}

    现在的incremtanl learning方法解决的优化问题是L2正则L2 loss的SVM优化问题，基于此在实际应用中得到的model会非常大，不适合使用。未来希望可以解决L1正则L2损失的SVM优化问题，可以得到稀疏模型，解决存储上的瓶颈。无论是L2正则或者L1正则的SVM优化问题，依然是单机上面的求解优化，随着数据规模的越来越大，即使是稀疏模型依然大到不能存储在单机上面，训练时间也会有不能接受的那一天，未来的工作可以提出一种分布式的解决方案,可以最终解决存储以及训练时间上的瓶颈。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《结论》作为最后一章
\chapter{结论}\label{chapter_concludes}

本文在第\ref{chapter_smallworld}章中，通过考虑数据中心网络布局构建中的最大度限制
问题，提出了符合数据中心网络基本要求的DS小世界模型，并分析了它的性质。随后提出
SIDN，将DS模型映射到具体的网络结构中，并分析了所构成网络的平均直径、网络总带宽、
对故障的容错能力等各项网络性能。

分析与仿真实验证明，SIDN网络具有很好的扩展能力，网络总带宽与网络规模成
近似线性增长的关系；具有很强的容错能力，链路损坏与节点损坏几乎无法破坏
网络的联通性，故障率对网络性能的影响与破坏节点/链路占总资源比率线性相关。

随后在第\ref{chapter_scalefree}章中，分析了无尺度网络在数据中心网络构建应用中的
理论方面问题。对Scafida \cite{gyarmati2010scafida}文中所述在最大度限制的情况下运
用BA算法构造的网络并不会损失无尺度性质的观点，进行了深入的分析，并指出了该论点的
局限性。

在给出了在引入节点最大度限制之后，利用分治和递归的思想，对无尺度网络
进行多层构建，对所构造的网络进行度-度相关性，以及聚类性分析。

\begin{table}
  \centering
  \begin{tabular}{cccp{38mm}}
    \toprule
    \textbf{文档域类型} & \textbf{Java类型} & \textbf{宽度(字节)} & \textbf{说明} \\
    \midrule
    BOOLEAN  & boolean &  1  & \\
    CHAR     & char    &  2  & UTF-16字符 \\
    BYTE     & byte    &  1  & 有符号8位整数 \\
    SHORT    & short   &  2  & 有符号16位整数 \\
    INT      & int     &  4  & 有符号32位整数 \\
    LONG     & long    &  8  & 有符号64位整数 \\
    STRING   & String  &  字符串长度  & 以UTF-8编码存储 \\
    DATE     & java.util.Date & 8 & 距离GMT时间1970年1月1日0点0分0秒的毫秒数 \\
    BYTE\_ARRAY & byte$[]$ & 数组长度 & 用于存储二进制值 \\
    BIG\_INTEGER & java.math.BigInteger & 和具体值有关 & 任意精度的长整数 \\
    BIG\_DECIMAL & java.math.BigDecimal & 和具体值有关 & 任意精度的十进制实数 \\
    \bottomrule
  \end{tabular}
  \caption{测试表格}\label{table:test5}
\end{table}

表\ref{table:test5}用于测试表格。随后分析了无尺度网络构造过程中，交换机节点与数
据节点的角色区别，分析了两者在不同比率下形成的网络形态，以及对网络性能造成的影响。

通过理论分析和仿真实验，分析并找出比率因子q的最佳取值。此外，无尺度现象
的引入提高了网络的聚类系数，从而在不失灵活性可靠性的基础上，进一步提升
了网络的性能。

在第\ref{chapter_random}章中，将关注点转移到交换机本身。由于图论难以描述数据中心
网络中的交换设备，因此放弃基于图的抽象模型，转而基于多维簇划分的思想，提出并设计
了WarpNet网络模型。

该网络模型突破了基于图描述的局限性，并对网络的带宽等指标进行理论分析并
给出定量描述。最后对比了理论分析、仿真测试结果，并在实际物理环境中进系
真实部署，通过6节点的小规模实验以及1000节点虚拟机的大规模实验，表明该模
型的理论分析、仿真测试与实际实验吻合，并在网络性能、容错能力、伸缩性灵
活性方面得到了进一步的提升。

在第\ref{chapter_experiments}章中，针对网络模型研究这一类工作的共性，设计构造通
用验证平台系统。以海量虚拟机和虚拟分布式交换机的形式，实现了基于少量物理节点，对
大规模节点的模拟。其模拟运行的过程与真实运行在实现层面完全一致，运行的结果与真实
环境线性相关。除为本文所涉若干网络模型提供验证外，可进一步推广到更为广泛的领域，
为各种网络模型及路由算法的研究工作，提供分析、指导与验证。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 致谢，应放在《结论》之后
\begin{acknowledgement}
  首先感谢我的母亲韦春花对我的支持。其次感谢我的导师陈近南对我的精心指导和热心帮助。接下来，
  感谢我的师兄茅十八和风际中，他们阅读了我的论文草稿并提出了很有价值的修改建议。

  最后，感谢我亲爱的老婆们：双儿、苏荃、阿珂、沐剑屏、曾柔、建宁公主、方怡，感谢
  你们在生活上对我无微不至的关怀和照顾。我爱你们！
\end{acknowledgement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 附录
\appendix

\chapter{博士(硕士)学位论文编写格式规定(试行)}

\section{适用范围}

本规定适用于博士学位论文编写，硕士学位论文编写应参照执行。

\section{引用标准}

GB7713科学技术报告、学位论文和学术论文的编写格式。

GB7714文后参考文献著录规则。

\section{印制要求}

论文必须用白色纸印刷，并用A4(210mm×297mm)标准大小的白纸。纸的四周应留足空白
边缘，上方和左侧应空边25mm以上，下方和右侧应空边20mm以上。除前置部分外，其它
部分双面印刷。

论文装订不要用铁钉，以便长期存档和收藏。

论文封面与封底之间的中缝（书脊）必须有论文题目、作者和学校名。

\section{编写格式}

论文由前置部分、主体部分、附录部分(必要时)、结尾部分(必要时)组成。

前置部分包括封面，题名页，声明及说明，前言，摘要(中、英文)，关键词，目次页，
插图和附表清单(必要时)，符号、标志、缩略词、首字母缩写、单位、术语、名词解释
表(必要时)。

主体部分包括绪论(作为正文第一章)、正文、结论、致谢、参考文献表。

附录部分包括必要的各种附录。

结尾部分包括索引和封底。

\section{前置部分}

\subsection{封面（博士论文国图版用）}

封面是论文的外表面，提供应有的信息，并起保护作用。

封面上应包括下列内容：
\begin{enumerate}
\item 分类号  在左上角注明分类号，便于信息交换和处理。一般应注明《中国图书资
  料分类法》的类号，同时应注明《国际十进分类法UDC》的类号；
\item 密级  在右上角注明密级；
\item “博士学位论文”用大号字标明；
\item 题名和副题名   用大号字标明；
\item 作者姓名；
\item 学科专业名称；
\item 研究方向；
\item 导师姓名，职称；
\item 日期包括论文提交日期和答辩日期；
\item 学位授予单位。
\end{enumerate}

\subsection{题名}

题名是以最恰当、最简明的词语反映论文中最重要的特定内容的逻辑组合。

题名所用每一词语必须考虑到有助于选定关键词和编写题录、索引等二次文献可以提供
检索的特定实用信息。

题名应避免使用不常见的缩略词、首字母缩写字、字符、代号和公式等。

题名一般不宜超过20字。

论文应有外文题名，外文题名一般不宜超过10个实词。

可以有副题名。

题名在整本论文中不同地方出现时，应完全相同。

\subsection{前言}

前言是作者对本论文基本特征的简介，如论文背景、主旨、目的、意义等并简述本论文
的创新性成果。

\subsection{摘要}

摘要是论文内容不加注释和评论的简单陈述。

论文应有中、英文摘要，中、英文摘要内容应相同。

摘要应具有独立性和自含性，即不阅读论文的全文，便能获得必要的信息，摘要
中有数据、有结论，是一篇完整的短文，可以独立使用，可以引用，可以用于推广。摘
要的内容应包括与论文同等量的主要信息，供读者确定有无必要阅读全文，也供文摘等
二次文献引用。摘要的重点是成果和结论。

中文摘要一般在1500字，英文摘要不宜超过1500实词。

摘要中不用图、表、化学结构式、非公知公用的符号和术语。

\subsection{关键词}

关键词是为了文献标引工作从论文中选取出来用于表示全文主题内容信息款目的单词或
术语。

每篇论文选取3－8个词作为关键词，以显著的字符另起一行，排在摘要的左下方。在英
文摘要的左下方应标注与中文对应的英文关键词。

\subsection{目次页}

目次页由论文的章、节、附录等的序号、名称和页码组成，另页排在摘要的后面。

\subsection{插图和附表清单}

论文中如图表较多，可以分别列出清单并置于目次页之后。

图的清单应有序号、图题和页码。表的清单应有序号、表题和页码。

符号、标志、缩略词、首字母缩写、计量单位、名词、术语等的注释表符号、标志、缩略词、
首字母缩写、计量单位、名词、术语等的注释说明汇集表，应置于图表清单之后。

\section{主体部分}

\subsection{格式}

主体部分由绪论开始，以结论结束。主体部分必须由另页右页开始。每一章必须另页开
始。全部论文章、节、目的格式和版面安排要划一，层次清楚。

\subsection{序号}

\begin{figure}[htbp]
  \centering
  \includegraphics[width= 0.5\textwidth]{njuname.eps}\\
  \caption{测试附录中的插图}\label{fig:appendix1}
\end{figure}

论文的章可以写成：第一章。节及节以下均用阿拉伯数字编排序号，如
1.1，1.1.1等。

论文中的图、表、附注、参考文献、公式、算式等一律用阿拉伯数字分别分章依序连续编排
序号。其标注形式应便于互相区别，一般用下例：图1.2；表2.3；附注1）；文献[4]；式
  (6.3)等。

论文一律用阿拉伯数字连续编页码。页码由首页开始，作为第1页，并为右页另页。封页、
封二、封三和封底不编入页码，应为题名页、前言、目次页等前置部分单独编排页码。页码
必须标注在每页的相同位置，便于识别。

\begin{equation}
    C_i = \frac{2E_i}{k_i(k_i-1)}
\end{equation}

附录依序用大写正体A、B、C、$\cdots$编序号，如：附录A。附录中的图、表、式、参考文
献等另行编序号，与正文分开，也一律用阿拉伯数字编码，但在数码前题以附条序码，如图
A.1；表B.2；式(B.3)；文献[A.5]等。

\subsection{绪论}

绪论（综述）：简要说明研究工作的目的、范围、相关领域的前人工作和知识空白、理
论基础和分析，研究设想、研究方法和实验设计、预期结果和意义等。一般在教科书中
有的知识，在绪论中不必赘述。

绪论的内容应包括论文研究方向相关领域的最新进展、对有关进展和问题的评价、本论
文研究的命题和技术路线等；绪论应表明博士生对研究方向相关的学科领域有系统深入
的了解，论文具有先进性和前沿性；

\begin{problem}
测试定理环境。测试定理环境。测试定理环境。测试定理环境。测试定理环境。测试定理环境。
测试定理环境。测试定理环境。测试定理环境。
\end{problem}

为了反映出作者确已掌握了坚实的基础理论和系统的专门知识，具有开阔的科学视野，对研
究方案作了充分论证，绪论应单独成章，列为第一章，绪论的篇幅应达$1\sim 2$万字，不
得少于$1$万字；绪论引用的文献应在$100$篇以上，其中外文文献不少于$60\%$；引用文献
应按正文中引用的先后排列。

\subsection{正文}

论文的正文是核心部分，占主要篇幅。正文必须实事求是，客观真切，准确完备，合乎
逻辑，层次分明，简便可读。

\begin{figure}[htbp]
  \centering
  \includegraphics[width= 0.5\textwidth]{njuname.eps}\\
  \caption{测试附录中的插图}\label{fig:appendix2}
\end{figure}

正文的每一章(除绪论外)应有小结，在小结中应明确阐明作者在本章中所做的工作，特
别是创新性成果。凡本论文要用的基础性内容或他人的成果不应单独成章，也不应作过
多的阐述，一般只引结论、使用条件等，不作推导。

\subsubsection{图}

图包括曲线图、构造图、示意图、图解、框图、流程图、记录图、布置图、地图、照片
、图版等。

图应具有“自明性”，即只看图、图题和图例，不阅读正文，就可以理解图意。

图应编排序号。每一图应有简短确切的图题，连同图号置于图下。必要时，应将图上的
符号、标记、代码，以及实验条件等，用最简练的文字，横排于图题下方，作为图例说
明。

\begin{example}
测试定理环境。测试定理环境。测试定理环境。测试定理环境。测试定理环境。测试定理环境。
测试定理环境。测试定理环境。测试定理环境。
\end{example}

曲线图的纵、横坐标必须标注“量、标准规定符号、单位”。此三者只有在不必要标明
(如无量纲等)的情况下方可省略。坐标上标注的量的符号和缩略词必须与正文一致。

照片图要求主题和主要显示部分的轮廓鲜明，便于制版。如用放大缩小的复制品，必须
清晰，反差适中。照片上应该有表示目的物尺寸的标度。

\subsubsection{表}

表的编排，一般是内容和测试项目由左至右横读，数据依序竖排。表应有自明性。

表应编排序号。

每一表应有简短确切的表题，连同标号置于表上。必要时，应将表中的符号、标记、代
码，以及需要说明事项，以最简练的文字，横排于表题下，作为表注，也可以附注于表
下。表内附注的序号宜用小号阿拉伯数字并加圆括号置于被标注对象的右上角，如：
xxx${}^{1)}$；不宜用“*”，以免与数学上共轭和物质转移的符号相混。

表的各栏均应标明“量或测试项目、标准规定符号、单位”。只有在无必要标注的情况下
方可省略。表中的缩略词和符号，必须与正文中一致。

表内同一栏的数字必须上下对齐。表内不宜用“同上”，“同左”和类似词，一律填入具体数字
或文字。表内“空白”代表未测或无此项，“－”或“\textellipsis”（因“－”可能与代表阴性
  反应相混）代表未发现，“0”代表实测结果确为零。

如数据已绘成曲线图，可不再列表。

\subsubsection{数学、物理和化学式}

正文中的公式、算式或方程式等应编排序号，序号标注于该式所在行(当有续行时，应
标注于最后一行)的最右边。

较长的式，另行居中横排。如式必须转行时，只能在$+$，$-$，$\times$，$\div$，$<$，
$>$处转行。上下式尽可能在等号“$=$”处对齐。

小数点用“$.$”表示。大于$999$的整数和多于三位数的小数，一律用半个阿拉伯数字符的小
间隔分开，不用千位撇。对于纯小数应将$0$列于小数点之前。

示例：应该写成$94\ 652.023\ 567$和$0.314\ 325$, 不应写成$94,652.023,567$和
$.314,325$。

应注意区别各种字符，如：拉丁文、希腊文、俄文、德文花体、草体；罗马数字和阿拉伯数
字；字符的正斜体、黑白体、大小写、上下脚标（特别是多层次，如“三踏步”）、上下偏差
等。

\subsubsection{计量单位}

报告、论文必须采用国务院发布的《中华人民共和国法定计量单位》，并遵照《中华人
民共和国法定计量单位使用方法》执行。使用各种量、单位和符号，必须遵循附录B所
列国家标准的规定执行。单位名称和符号的书写方式一律采用国际通用符号。

\subsubsection{符号和缩略词}

符号和缩略词应遵照国家标准的有关规定执行。如无标准可循，可采纳本学科或本专业
的权威性机构或学术团体所公布的规定；也可以采用全国自然科学名词审定委员会编印
的各学科词汇的用词。如不得不引用某些不是公知公用的、且又不易为同行读者所理解
的、或系作者自定的符号、记号、缩略词、首字母缩写字等时，均应在第一次出现时一
一加以说明，给以明确的定义。

\subsection{结论}

报告、论文的结论是最终的、总体的结论，不是正文中各段的小结的简单重复。结论应
该准确、完整、明确、精炼。在结论中要清楚地阐明论文中有那些自己完成的成果，特
别是创新性成果；

如果不可能导出应有的结论，也可以没有结论而进行必要的讨论。可以在结论或讨论中
提出建议、研究设想、仪器设备改进意见、尚待解决的问题等。

\subsection{致谢}

可以在正文后对下列方面致谢：

\begin{itemize}
\item 国家科学基金、资助研究工作的奖学金基金、合作单位、资助或支持的企业、组织或个
人；
\item 协助完成研究工作和提供便利条件的组织或个人；
\item 在研究工作中提出建议和提供帮助的人；
\item 给予转载和引用权的资料、图片、文献、研究思想和设想的所有者；
\item 其他应感谢的组织或个人。
\end{itemize}

\subsection{参考文献表}

\subsubsection{专著著录格式}

主要责任者，其他责任者，书名，版本，出版地：出版者，出版年

例：1. 刘少奇，论共产党员的修养，修订2版，北京：人民出版社，1962

\subsubsection{连续出版物中析出的文献著录格式}

析出文献责任者，析出文献其他责任者，析出题名，原文献题名，版本：文献中的位置。

例：2. 李四光，地壳构造与地壳运动，中国科学，1973 (4)：400－429

参考文献采用顺序编码制，按论文正文所引用文献出现的先后顺序连续编码。

\section{附录}

附录是作为报告、论文主体的补充项目，并不是必需的。

下列内容可以作为附录编于报告、论文后，也可以另编成册；

\begin{enumerate}
\item 为了整篇论文材料的完整，但编入正文又有损于编排的条理和逻辑性，这一材料
包括比正文更为详尽的信息、研究方法和技术更深入的叙述，建议可以阅读的参考文献
题录，对了解正文内容有用的补充信息等；
\item 由于篇幅过大或取材于复制品而不便于编入正文的材料；
\item 不便于编入正文的罕见珍贵资料；
\item 对一般读者并非必要阅读，但对本专业同行有参考价值的资料；
\item 某些重要的原始数据、数学推导、计算程序、框图、结构图、注释、统计表、计
算机打印输出件等。
\end{enumerate}

附录与正文连续编页码。

每一附录均另页起。

\section{结尾部分 (必要时)}

为了将论文迅速存储入电子计算机，可以提供有关的输入数据。可以编排分类索引、著者索
引、关键词索引等。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 书籍附件
\backmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 参考文献。应放在\backmatter之后。

% 推荐使用BibTeX，若不使用BibTeX时注释掉下面一句。
\nocite{*}
\bibliography{sample}

% 不使用 BibTeX
%\begin{thebibliography}{2}
%
%\bibitem{deng:01a}
%{邓建松,彭冉冉,陈长松}.
%\newblock {\em \LaTeXe{}科技排版指南}.
%\newblock 科学出版社,书号:7-03-009239-2/TP.1516, 北京, 2001.
%
%\bibitem{wang:00a}
%王磊.
%\newblock {\em \LaTeXe{}插图指南}.
%\newblock 2000.
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 作者简历与科研成果页，应放在参考文献之后
\begin{resume}
% 论文作者身份简介，一句话即可。
\begin{authorinfo}
\noindent 韦小宝，男，汉族，1985年11月出生，江苏省扬州人。
\end{authorinfo}
% 论文作者教育经历列表，按日期从近到远排列，不包括将要申请的学位。
\begin{education}
\item[2007年9月 --- 2010年6月] 南京大学计算机科学与技术系 \hfill 硕士
\item[2003年9月 --- 2007年6月] 南京大学计算机科学与技术系 \hfill 本科
\end{education}
% 论文作者在攻读学位期间所发表的文章的列表，按发表日期从近到远排列。
\begin{publications}
\item Xiaobao Wei, Jinnan Chen, ``Voting-on-Grid Clustering for Secure
  Localization in Wireless Sensor Networks,'' in \textsl{Proc. IEEE International
    Conference on Communications (ICC) 2010}, May. 2010.
\item Xiaobao Wei, Shiba Mao, Jinnan Chen, ``Protecting Source Location Privacy
  in Wireless Sensor Networks with Data Aggregation,'' in \textsl{Proc. 6th
    International Conference on Ubiquitous Intelligence and Computing (UIC)
    2009}, Oct. 2009.
\end{publications}
% 论文作者在攻读学位期间参与的科研课题的列表，按照日期从近到远排列。
\begin{projects}
\item 国家自然科学基金面上项目``无线传感器网络在知识获取过程中的若干安全问题研究''
（课题年限~2010年1月 --- 2012年12月），负责位置相关安全问题的研究。
\item 江苏省知识创新工程重要方向项目下属课题``下一代移动通信安全机制研究''
（课题年限~2010年1月 --- 2010年12月），负责LTE/SAE认证相关的安全问题研究。
\end{projects}
\end{resume}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成《学位论文出版授权书》页面，应放在最后一页
\makelicense

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
