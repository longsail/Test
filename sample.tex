%% 使用 njuthesis 文档类生成南京大学学位论文的示例文档
%%
%% 作者：胡海星，starfish (at) gmail (dot) com
%% 项目主页: http://haixing-hu.github.io/nju-thesis/
%%
%% 本样例文档中用到了吕琦同学的博士论文的提高和部分内容，在此对他表示感谢。
%%
\documentclass[master]{njuthesis}
%% njuthesis 文档类的可选参数有：
%%   nobackinfo 取消封二页导师签名信息。注意，按照南大的规定，是需要签名页的。
%%   phd/master/bachelor 选择博士/硕士/学士论文

% 使用 blindtext 宏包自动生成章节文字
% 这仅仅是用于生成样例文档，正式论文中一般用不到该宏包
\usepackage[math]{blindtext}
\usepackage{algorithm}
\usepackage{algpseudocode}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置《国家图书馆封面》的内容，仅博士论文才需要填写

% 设置论文按照《中国图书资料分类法》的分类编号
\classification{0175.2}
% 论文的密级。需按照GB/T 7156-2003标准进行设置。预定义的值包括：
% - \openlevel，表示公开级：此级别的文献可在国内外发行和交换。
% - \controllevel，表示限制级：此级别的文献内容不涉及国家秘密，但在一定时间内
%   限制其交流和使用范围。
% - \confidentiallevel，表示秘密级：此级别的文献内容涉及一般国家秘密。
% - \clasifiedlevel，表示机密级：此级别的文献内容涉及重要的国家秘密 。
% - \mostconfidentiallevel，表示绝密级：此级别的文献内容涉及最重要的国家秘密。
% 此属性可选，默认为\openlevel，即公开级。
\securitylevel{\controllevel}
% 设置论文按照《国际十进分类法UDC》的分类编号
% 该编号可在下述网址查询：http://www.udcc.org/udcsummary/php/index.php?lang=chi
\udc{004.72}
% 国家图书馆封面上的论文标题第一行，不可换行。此属性可选，默认值为通过\title设置的标题。
\nlctitlea{自然语言处理组}
% 国家图书馆封面上的论文标题第二行，不可换行。此属性可选，默认值为空白。
\nlctitleb{面向短文本主题发现及分类研究}
% 国家图书馆封面上的论文标题第三行，不可换行。此属性可选，默认值为空白。
\nlctitlec{}
% 导师的单位名称及地址
\supervisorinfo{南京大学计算机科学与技术系~~南京市汉口路22号~~210093}
% 答辩委员会主席
%\chairman{XX~~教授}
% 第一位评阅人
%\reviewera{阳顶天~~教授}
% 第二位评阅人
%\reviewerb{张无忌~~副教授}
% 第三位评阅人
%\reviewerc{黄裳~~教授}
% 第四位评阅人
%\reviewerd{郭靖~~研究员}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文封面

% 论文标题，不可换行
\title{面向短文本主题发现及分类研究}
% 论文作者姓名
\author{邹远航}
% 论文作者联系电话
\telphone{15850725926}
% 论文作者电子邮件地址
\email{zouyh@nju.edu.cn}
% 论文作者学生证号
\studentnum{MF1233059}
% 论文作者入学年份（年级）
\grade{2012}
% 导师姓名职称
\supervisor{戴新宇~~副教授}
% 导师的联系电话
\supervisortelphone{13671607471}
% 论文作者的学科与专业方向
\major{计算机技术}
% 论文作者的研究方向
\researchfield{自然语言处理}
% 论文作者所在院系的中文名称
\department{计算机科学与技术系}
% 论文作者所在学校或机构的名称。此属性可选，默认值为``南京大学''。
\institute{南京大学}
% 论文的提交日期，需设置年、月、日。vvvvvv
\submitdate{2015年4月15日}
% 论文的答辩日期，需设置年、月、日。
\defenddate{2015年6月1日}
% 论文的定稿日期，需设置年、月、日。此属性可选，默认值为最后一次编译时的日期，精确到日。
%% \date{2013年5月1日}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文封面

% 论文的英文标题，不可换行
\englishtitle{Machine learning Algorithm in short text}
% 论文作者姓名的拼音
\englishauthor{ZOU Yuan-Hang}
% 导师姓名职称的英文
\englishsupervisor{Professor DAI Xin-Yu}
% 论文作者学科与专业的英文名
\englishmajor{Computer Technology}
% 论文作者所在院系的英文名称
\englishdepartment{Department of Computer Science and Technology}
% 论文作者所在学校或机构的英文名称。此属性可选，默认值为``Nanjing University''。
\englishinstitute{Nanjing University}
% 论文完成日期的英文形式，它将出现在英文封面下方。需设置年、月、日。日期格式使用美国的日期
% 格式，即``Month day, year''，其中``Month''为月份的英文名全称，首字母大写；``day''为
% 该月中日期的阿拉伯数字表示；``year''为年份的四位阿拉伯数字表示。此属性可选，默认值为最后
% 一次编译时的日期。
\englishdate{March 23, 2015}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文摘要

% 设置中文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\title|命令所设置的论文标题
% \abstracttitlea{数据中心网络模型研究}
% 设置中文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
% \abstracttitleb{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文摘要

% 设置英文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\englishtitle|命令所设置的论文标题
\englishabstracttitlea{A Research on Network Infrastructures}
% 设置英文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
\englishabstracttitleb{for Data Centers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 制作国家图书馆封面（博士学位论文才需要）
\makenlctitle
% 制作中文封面
\maketitle
% 制作英文封面
\makeenglishtitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始前言部分
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的中文摘要
\begin{abstract}
随着社交网络的发展，智能手机的普及，在社交网络上比如新浪微博，QQ，在智能手机的语音助手上比如Cortana，Siri上产生了非常多的数据。
这些数据有着相同的特点，文本长度比较短。然语言处理算法在文章和新闻等长文本上的任务如文本主题发现和文本分类任务上比较有效。
。由于短文本长度较短，信息量少， 文本更新速度快，由此带来的挑战有：1）使用传统的特征提取方法比如one-hot，bag of words方法提取的特征比较稀疏，2）对于每时每刻更新的大规模数据，重新开始训练模型时间代价很大。
本文针对上述传统自然语言处理方法的问题，具体地，尝试对短文本上的微博主题发现，领域查询分类以及基于查询分类的增量式学习等任务进行深入研究并且在相关数据集上进行实验验证。对于微博短文本主题发现采用LDA分解获取话题信息从而可以丰富特征，对于领域查询分类查询引入slot实体丰富特征，对于大规模数据的训练采用增量式学习方法来减少训练时间。在三个任务上我们都取得了不错的实验结果。相比于传统方法，我们的方法在三个任务上都取得了更好的实验结果。

% 中文关键词。关键词之间用中文全角分号隔开，末尾无标点符号。
\keywords{自然语言处理；短文本；主题发现；分类；增量式学习}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的英文摘要
\begin{englishabstract}
%\blindtext
% 英文关键词。关键词之间用英文半角逗号隔开，末尾无符号。
With the rapid development of social network and the prevalence of smartphones, an increasingly large volumn of data are produced throughout the social network platforms like Sina Microblog, qq as well as speech assistant like Cortana and Siri. There are many similarities which they share in common. For example, the text lengths are short. Related machine learning algorithms have witnessed great success in processing traditional long text but for short text processing, there are still many challenges lying ahead. This paper includes comprehensive and in-depth research in key phrase extraction, query classification and incremental learning in an attempt to address the issues in short text processing and also we conducted experiments on our dataset to validate its effectiveness. The experimental results indicate that our approach yields satisfactory results.  

\englishkeywords{Machine Learning, Short Text, Keyphrase Extraction, classification}
\end{englishabstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的前言，应放在目录之前，中英文摘要之后
%
\begin{preface}

随着社交网络的发展以及智能手机的普及，在这些平台上产生的短文本数据量越来越多。他们较之普通文本的特点是，
文本长度比较短，信息量少以及文本主题多样化，文本数据量更新速度快。因此带来的挑战有，一方面一般的特征提取模型比如one-hot\cite{one-hot}, bag-of-words\cite{bag-of-words}可以提取的特征比较稀疏，另一方面每时每刻都在产生巨大的数据，如何利用这些大规模数据训练新的模型。自然语言处理算法在传统文本上的应用比较有效，如何改进自然语言处理算法使之可以更好地处理短文本数据值得研究。本文尝试在短文本主题词发现，领域查询分类以及增量式学习在分类中的应用三个任务上进行深入研究。对于短文本主题词发现，使用LDA模型分解获得隐藏话题信息来丰富特征，从而可以提高短文本主题词发现的F值；对于领域查询分类来说，主动引入slot实体来丰富特征，从而可以提高分类精度；对于增量式学习来说，通过使用之前模型训练的参数来初始化新的模型，从而可以减少训练时间。
下面将分别阐述，

\begin{enumerate}

    \item 短文本主题词发现是指给定包含一个关键词的大量文本，从中发现若干主题词集合。短文本主题词发现的一般方法是，首先使用正则表达式提取出名词短语，然后基于TFIDF来计算名词短语的权重，最后根据权重排序来提取主题短语。然而，对于微博文本，由于微博长度短并且信息量低，同时微博表达的主题多样化，基于此我们使用了基于LDA以及PageRank模型的短文本主题发现方法。具体来说，通过对微博使用话题模型分解获取话题分布以及话题下面的单词分布，同时配合PageRank算法，对每一个话题下面的单词排序，最终可以得到每个话题下面的单词排序。通过使用正则表达式抽取名词短语，考虑微博的话题分布同时对短语中每个词在每一个话题下面的权重分布求和最终获取名词短语的排序，从而抽取出对应的短文本主题词。

    \item 领域查询分类是指给定一个查询，将此查询正确的分类到相应的领域。领域查询分类的一般方法是提取bag-of-ngrams的特征然后使用分类器例如SVM进行分类。然而由于查询长度比较短同时部分查询具有歧义导致可以提取的特征非常少，特征非常稀疏，基于此我们提出了基于slot的领域查询分类方法。具体俩说，通过引入slot实体来消除部分查询歧义同时丰富查询的特征；通过对每一个领域训练CRF模型，给未知的查询分别标注slot，抽取包含slot的特征，最后使用SVM分类器进行分类。我们的实验结果表明结合slot之后的分类模型可以提高领域查询分类的准确度。

    \item 使用新增加的数据提升模型分类准确度的一般做法都是将原始数据跟新增数据结合在一起，重新训练模型。然而随着数据量越来越多，如果每次都是重新训练模型，将会有很大的时间代价。增量式学习是指当增加新的数据时候，通过使用前一个模型的最优参数来初始化数据更新之后的模型参数形成新的优化问题，接着使用相应的优化算法来求解新的优化问题，得到最终的训练模型。我们的实验结果表明相比于重新训练模型当有新的数据增加时候，使用增量式学习算法，可以在保持准确率一致的情况下训练时间大大减少。
\end{enumerate}


\vspace{1cm}
\begin{flushright}
邹远航\\
2015年夏于南京大学
\end{flushright}

\end{preface}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成论文目次
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成插图清单。如无需插图清单则可注释掉下述语句。
\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成附表清单。如无需附表清单则可注释掉下述语句。
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始正文部分
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《绪论》作为第一章

\chapter{绪论}\label{chapter_introduction}

\section{研究背景}
    
    文本任务（文本分析）是自然语言处理领域的一项非常重要的问题，诸如主题词提取，文本分类等等。传统的特征提取模型比如one-hot模型\cite{one-hot},bag-of-words模型\cite{bag-of-words},结合TFIDF方法\cite{keyword}以及SVM模型\cite{documentC}可以在这些任务上取得不错的结果。然而随着社交网络的兴起以及智能手机的普及，微博或者语音查询等新型文本形式出现，他们较之普通文本的特点是，文本长度比较短，蕴含的信息量比较少同时伴随着文本数量更新比较快。现有方法在处理这些新型文本时不足之处有，对于特征提取，现有方法一般是直接提取文本特征(one-hot，bag-of-words等等）导致特征比较稀疏；对于模型训练，现有方法一般是重新开始训练模型（从零开始训练)导致模型训练时间比较长。在接下来的研究内容里面，我们试图解决前面存在的问题，具体我们介绍3个任务，分别是短文本主题词发现，领域查询分类，以及增量式学习。对于短文本主题词发现，使用LDA模型分解获得隐藏话题信息来丰富特征，从而可以提高短文本主题词发现的F值；对于领域查询分类来说，主动引入slot实体来丰富特征，从而可以提高分类精度；对于增量式学习来说，通过使用之前模型训练的参数来初始化新的模型，从而可以减少训练时间。

\section{研究内容}

    短文本主题词发现是指给定包含一个关键词的大量文本，从中发现若干主题词集合。短文本主题词发现的一般方法是，首先使用正则表达式提取出名词短语，然后基于TFIDF来计算名词短语的权重，最后根据权重排序来提取主题短语。然而，对于微博文本，由于微博长度短并且信息量低，同时微博表达的主题多样化，基于此我们使用了基于LDA以及PageRank模型的短文本主题发现方法。具体来说，通过对微博使用话题模型分解获取话题分布以及话题下面的单词分布，同时配合PageRank算法，对每一个话题下面的单词排序，最终可以得到每个话题下面的单词排序。通过使用正则表达式抽取名词短语，考虑微博的话题分布同时对短语中每个词在每一个话题下面的权重分布求和最终获取名词短语的排序，从而抽取出对应的短文本主题词。

    领域查询分类是指给定一个查询，将此查询正确的分类到相应的领域。领域查询分类的一般方法是提取bag-of-ngrams的特征然后使用分类器例如SVM进行分类。然而由于查询长度比较短同时部分查询具有歧义导致可以提取的特征非常少，特征非常稀疏，基于此我们提出了基于slot的领域查询分类方法。具体俩说，通过引入slot实体来消除部分查询歧义同时丰富查询的特征；通过对每一个领域训练CRF模型，给未知的查询分别标注slot，抽取包含slot的特征，最后使用SVM分类器进行分类。我们的实验结果表明结合slot之后的分类模型可以提高领域查询分类的准确度。

    使用新增加的数据提升模型分类准确度的一般做法都是将原始数据跟新增数据结合在一起，重新训练模型。然而随着数据量越来越多，如果每次都是重新训练模型，将会有很大的时间代价。增量式学习是指当增加新的数据时候，通过使用前一个模型的最优参数来初始化数据更新之后的模型参数形成新的优化问题，接着使用相应的优化算法来求解新的优化问题，得到最终的训练模型。我们的实验结果表明相比于重新训练模型当有新的数据增加时候，使用增量式学习算法，可以在保持准确率一致的情况下训练时间大大减少。

\section{论文组织}

    本文的组织如下，第一章介绍基于LDA和PageRank的方法在微博主题词发现中的应用，首先介绍主题词发现的研究背景；其次介绍主题词发现两种算法，基于TFIDF的主题词发现算法以及基于LDA和PageRank的主题词发现算法，在算法设计过程中使用了LDA以及PageRank模型，我们将会首先对此两种模型做简单的介绍；然后简略介绍了微博主题词发现系统的架构，最后是实验结果分析以及总结。

    第二章介绍slot实体特征在领域查询分类中的应用，首先介绍领域查询分类的背景；其次介绍领域查询分类的两种算法，基于generic slot tagger的分类模型以及基于in-doman slot tagger的分类模型，在算法设计过程中使用了SVM以及CRF模型，我们将会首先对此两种模型做简单的介绍；最后是实验结果以及分析，在实验中我们分别对比了没有使用slot的分类模型与基于generic slot tagger的分类模型以及基于in-doman的分类模型的准确率。

    第三章介绍增量式学习在领域查询分类中的应用，首先介绍增量式学习的研究背景；其次介绍增量式学习与领域查询分类中使用的SVM模型中的原始问题以及对偶问题的联系，然后介绍增量式学习中使用的两种优化算法，坐标梯度优化算法以及置信域牛顿优化算法；最后是实验结果以及分析，在实验中我们对比了增量式学习与非增量式学习算法在领域查询分类任务中的准确度以及训练时间。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{微博短文本主题发现}\label{chapter_keyword}
\section{研究背景}

    近些年来随着社交网络的兴起，基于社交网络的应用越来越多。在中国新浪微博的用户数量活跃度最高，随着新浪微博的使用越来越来普及，新浪微博每时每刻都在产生大量的数据，如何利用以及分析这些大规模数据是一件非常有意义的事情。当然现在基于新浪微博上的上的应用已经非常丰富了，例如政府可以对于社会热点事件进行舆情分析，企业主可以实时获取自己的产品在微博上的反馈，广告主可以根据用户过去的浏览以及关注历史向用户定向推荐广告。然而无论是舆情分析，产品反馈亦或是广告推荐，文本主题词对于这些应用来说都会有很大的帮助。我们希望基于给定包含一个关键词的大量文本，发现若干主题词集合，这些主题词可以用于舆情分析，产品反馈，广告推荐，同时这些主题词还可以尝试用来做微博上的的分类，推荐以及用于预测微博上的热点事件发生等等应用。

    现有的自然语言处理算法在传统文本上的主题词发现任务上以及比较有效。对于新浪微博面临以下问题，微博长度比传统的文本短，有一部分微博没有有用的信息同时微博上的主题更加多样化。我们希望在现有的自然语言处理算法基础上结合微博的特点实现主题词发现算法。同时伴随着可视化技术的发展以及可视化可以给用户带来直观的效果，我们将自己处理的结果使用可视化工具呈现出来。基于此我们实现了基于LDA以及PageRank的微博主题词发现系统。

    本章接下来的组织如下，首先我们将阐述主题词发现的相关工作TFIDF等方法。其次我们将详细介绍基于LDA和PageRank的主题词发现算法。然后简略的介绍系统架构，接着描述实验设计以及结果，最后总结本章。

\section{主题词发现介绍}

 在介绍主题词之前首先介绍关键词提取的概念。关键词提取指的是从一篇或者一批文档中提取出来的词语，这些词语可以概括文档的中心思想，被广泛的用在建立索引，文档概要等应用中。主题词指的是由两个及以上的词语组成基于关键词提取的方法，不再关注提取单一的词语而是将关注点放在提取名词短语上。主题词具有比关键词更强的描述能力。

    主题词发现的方法可以分为有监督学习的方法以及无监督学习的方法，接下来将简略的介绍有监督学习以及无监督学习的概念。有监督学习是需要人工标注一批数据，然后将数据划分成训练集以及测试集，利用机器学习的算法在训练集上训练出模型，在测试集上测试模型的效果进而反馈给训练方法修改模型参数，使得最终的模型具有很好的泛化能力。有监督学习中最普遍的一类机器学习算法就是分类以及回归算法。无监督学习指的是不需要人工标注数据，让计算机自己去学习，无监督学习算法中最普遍的一类算法就是聚类算法。

    在主题词发现中，有监督学习的方法将主题词提取看作是一个分类的任务，通过使用语料集训练模型来判断候选的主题词是否是真正的主题词，在无监督方法中基于图的排序算法是学术上效果最好的方法。在当今时代，互联网发展迅速可以很容易方便的获取大量的数据，如果对大规模的数据进行标注，需要付出很大的人工成本以及时间代价，在实际工作中几乎是不可能完成的，所以有监督的方法不太实用，因此基于无监督学习的方法越来越受到重视。

    接下来我们将介绍基于TFIDF模型的主题词发现算法以及清华大学[Zhiyuan Liu]\cite{keyphrase}提出的基于LDA以及PageRank的主题词发现算法。

\section{基于TFIDF的主题词发现算法}

    TFIDF是一种统计方法，在信息检索中使用比较广泛，可以用来评估一个词对于一个文件集合的重要程度。TFIDF主要思想就是，如果某个词在一篇文档中出现的频率高，在其它文档中出现的次数比较少，就认为这个词具有很好的区分能力。TFIDF中TF（term frequency）指的是某个词在文档中出现的频率，IDF（inverse document frequency）指的是如果包含某个词条t的文档n数目很少，那么IDF就越大，同时可以说明词条t具有很好的区分能力。

    TFIDF的计算公式是TFIDF= TF*IDF，词频(TF)指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数的归一化，以防止它偏向长的文件。[copy],对于在某一个特定文件里的词语$t_i$来说，它的重要性可表示为$tf_i,_j = \frac{n_i,_j}{\sum_{k} n_k,_j}$，以上$n_i,_j$是该词在文件$d_j$中的出现次数，而分母则是在文件$d_j$中所有字词的出现次数之和。逆向文件频率(IDF)是一个词语普遍重要性的度量，可以表示为$idf_i = log\frac{\vert D \vert}{\vert \{j:t_i \in d_j \}\vert}$，其中$\vert D \vert:$ 是语料库中的文件总数。$\vert\{ j:t_i \in d_j\}\vert:$，包含词语$t_i$的文件数目。

    基于TFIDF的主题词提取算法流程如下，\\

\fbox{\begin{minipage}{32em}
  \begin{enumerate}
    \item 将所有微博合并至一篇文档
    \item 使用正则表达式提取文档中的名词短语
    \item 计算名词短语的TFIDF值，进行排序
    \item 提取top N的名词短语作为主题词
  \end{enumerate}
 \end{minipage}}
\\

    TFIDF的优点是实现简单并且效果不错因而在实际中使用非常广泛，缺点就是TFIDF算法中并没有体现出单词的位置信息，因此存在改进的机会。

\section{基于LDA以及PageRank的主题词发现算法}

    在无监督方法中，基于图的排序算法是效果最好的方法，这些方法首先根据文档中词之间的联系构造图，然后利用随机游动（例如PageRank）方法来预测词语的重要性，然后排序靠前的词语被选为主题词。然而，这些方法没有考虑到对于一个文档来说通常包含几个主题语义。对于给定的文档，好的主题词是跟文档主题相关并且主题词集合可以覆盖文档的主题，然而对于基于图的排序算法，仅仅考虑词语之间的联系，排序靠前的词语联系比较紧密通常属于一个主题，并且有一些词语跟主题无关。因此考虑引入主题模型来缓解这一问题，将传统的Pagerank根据不同主题分解为多个PageRank，然后根据不同主题下面的词语的排序获取主题词。

    基于LDA以及PageRank的主题词发现算法\cite{keyphrase}是清华大学刘知远等老师提出的算法，基本流程是，首先基于微博构建词图(word graph)，后面会介绍词图的构建方法。其次基于LDA模型，给定n个主题，在词图上得到n个主题的概率分布以及每个单词在每一个主题下的概率值。接着基于词图,文档的主题分布,每一个主题下单词的概率值，在词图(word graph)上运行改进的PageRank算法。由于改进的PageRank算法可以利用文档的主题分布以及n个不同的主题下的单词分布，经过迭代收敛之后可以获取n个具有已经排好序的单词的词图。接着使用基于LDA以及PageRank的主题词发现算法提取主题词。

   本节的组织如下，首先介绍算法中涉及到的LDA以及PageRank模型，然后介绍基于LDA以及PageRank的主题词发现算法。

\subsection{LDA模型介绍}

    LDA是一种典型的概率生成模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系\cite{LDA}。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。正如Beta分布是二项式分布的共轭先验概率分布，狄利克雷分布作为多项式分布的共轭先验概率分布。因此正如LDA‎贝叶斯网络结构中所描述的，在LDA模型中一篇文档生成的方式如下，\\
\fbox{\begin{minipage}{32em}
  \begin{enumerate}
    \item 从狄利克雷分布$\alpha$中取样生成文档i的主题分布$\theta_i$
    \item 从主题的多项式分布$\theta_i$中取样生成文档i第j个词的主题$z_{i, j}$
    \item 从狄利克雷分布$\beta$中取样生成主题$z_{i, j}$的词语分布$\phi_{z_{i, j}}$
    \item 从词语的多项式分布$\phi_{z_{i, j}}$中采样最终生成词语$w_{i, j}$
  \end{enumerate}
 \end{minipage}}
\\

    因此整个模型中所有可见变量以及隐藏变量的联合分布是

    $p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) = \prod_{j = 1}^{N} p(\theta_i|\alpha)p(z_{i, j}|\theta_i)p(\Phi|\beta)p(w_{i, j}|\theta_{z_{i, j}})$

    最终一篇文档的单词分布的最大似然估计可以通过将上式的$\theta_i$以及$\Phi$进行积分和对$z_i$进行求和得到

    $p(w_i | \alpha, \beta)  = \int_{\theta_i}\int_{\Phi }\sum_{z_i}p(w_i, z_i, \theta_i, \Phi | \alpha, \beta)$

    根据$p(w_i | \alpha, \beta)$的最大似然估计，最终可以通过吉布斯采样等方法估计出模型中的参数，下面简单的介绍吉布斯采样算法，
    
    Gibbs Sampling算法的运行方式是每次选取概率向量的一个维度，给定其他维度的变量值采样当前维度的值。不断迭代，直到收敛输出待估计的参数，初始时随机给文本中的每个单词分配主题$z\left(0\right)$，然后统计每个主题z下出现term t的数量以及每个文档m下出现主题z中的词的数量，每一轮计算$p\left(z_i|z_{-i},d,w\right)$，即排除当前词的主题分配：根据其他所有词的主题分配估计当前词分配各个主题的概率。当得到当前词属于所有主题z的概率分布后，根据这个概率分布为该词采样一个新的主题。然后用同样的方法不断更新下一个词的主题，直到发现每个文档下Topic分布$\theta$和每个Topic下词的分布$\phi$收敛，算法停止，输出待估计的参数$\theta$和$\phi$，最终每个单词的主题$z_{mn}$也同时得出。
实际应用中会设置最大迭代次数。每一次计算p(zi|z-i,d,w)的公式称为Gibbs updating rule。

\subsection{PageRank模型介绍}

    PageRank算法是Google用来进行对网页进行排序的算法\cite{PageRank}，PageRank算法是一种链接分析的算法，它将会给予超链接网页集合中的每一个网页一个数值权重，这个权重可以用来衡量此网页在超链接网页集合中的重要性。PageRank算法的做法首先将网页建模成图，图中的每个节点(网页)都有入度以及出度，每一个点的权重是由指向这些点来决定的，然后再加上一项是随机因子，保证每个节点有相同的概率挑往其他节点。经过迭代之后每一个节点可以得到稳定的权值。PageRank每个节点的入度是指向此节点的节点数量，每个节点的出度指的是当前节点指向的其它节点的数量。

    PageRank算法输出一个概率分布，用来表示人们随机点击一个链接的概率。Pagerank算法的数学公式表示是，$ PR\left(p_i\right) = \frac{1-d}{N}+d\sum_{p_j \in M\left(p_i\right)} \frac{PR\left(p_j\right)}{L\left(p_j\right)}$$p_1,p_2,...,p_N$是被研究的页面，$M\left(p_i\right)$是链入$p_i$的页面的集合， $L\left(p_j\right)$是$p_j$链出页面的数量，$N$是所有页面的集合。

\subsection{主题词发现算法}

    基于LDA和PageRank的主题词发现算法中使用的LDA模型以及PageRank模型已经在上面两节中介绍过了。下面我们阐述具体的算法。

    首先在微博上运行LDA算法，得到微博上的n个主题分布以及每个主题下面的单词分布。我们已经在上面的章节中介绍过LDA模型。其次基于微博构建词图，在词图上运行修改后的PageRank算法，原始的PageRank每个节点跳往其他节点是概率是相同的，这里做出的修改是，将这一项修改为单词(节点)在某个topic下面的概率分布，这样单词在跳转时候会倾向于跳转跟此topic相关的词语，我们使用$G=\left(V,E\right)$来表示有文档表示的图，其中的节点集合$V=\{w_1,w_2,\dots,w_N\}$，边的集合$E$,$\left(w_i,w_j\right) \in E$如果$w_i$和$w_j$之间有一条链接，在词图中每一个单词表示节点，每一条边表示单词之间的边，接下来我们使用$e\left(w_i,w_j\right)$来表示链接$\left(w_i,w_j\right)$之间的权重，节点$w_i$的初度表示为$O\left(w_i\right) = \sum_{j:w_i \rightarrow w_j} e\left(w_i,w_j\right)$，在原始的PageRank中单词$w_i$的得分$R\left(w_i\right)$的数学公式是
  
    $R\left( w_i \right) = \lambda \sum_{j:w_j \rightarrow w_i} \frac{e\left(w_j,w_i\right)}{O\left(w_j\right)}+\left(1-\lambda\right)\frac{1}{\vert V \vert}$，

    其中$\lambda$指的是衰减因子，取值范围0到1，$\vert V \vert$指的是节点的总数，衰减因子暗示每一个节点都有相同的概率$\left(1-\lambda\right)\frac{1}{\vert V \vert}$跳转到其它的节点。修改后的PageRank数学公式是

    $R_z\left( w_i \right) = \lambda \sum_{j:w_j \rightarrow w_i} \frac{e\left(w_j, w_i\right)}{O\left(w_j\right)}R_z\left(w_j\right)+\left( 1-\lambda \right)p_z\left(w_i\right)$。
 
    这里将$\left(1-\lambda\right)\frac{1}{\vert V \vert}$修改为$\left( 1-\lambda\right)p_z\left(w_i\right)$。这里跳转的概率不再是一致的，而是使用了单词在某一个主题下面的概率，这样节点在跳转时会倾向于跳转到同一个主题下面的单词。在词图上运行改进的PageRank算法之后，可以得到每个主题下面的单词排序。基于主题词一般是名词短语这个假设，对经过分词之后的语料进行词性标注，词性标注指的是对于句子中的每个词都指派一个合适的词性，也就是要确定每个词是名词、动词、形容词或者其它词性的过程。[copy]接着使用正则表达式$\left(adjective\right)*\left(noun\right)+$抽取所有的名词短语，对于某一个名词短语首先计算一个主题下面的权重，使用的公式如下$R_z\left(p\right) = \sum_{w_i \in p} R_z\left(w_i\right)$，这里的$p$指的是名词短语，$w_i$指的是名词短语中的单词，$R_z\left(w_i\right)$指的是单词$w_i$在主题$z$上的经过排序之后的权重。考虑到文档中的n个主题分布，那么名词短语的最终权重需要考虑n个不同的主题分布，最终使用下面这个公式对所有的主题词排序，$R\left(p\right) = \sum_{z=1}^k R_z\left(p\right) \times pr\left(z \vert d \right)$。这里的$R_z\left(p\right)$指的是名词短语在一个主题下的权重，$pr\left(z\vert d\right)$指的是某个主题在文档的概率分布，$R\left(p\right)$指的是考虑了文档中的主题分布之后，名词短语最终的权重。

\section{微博主题词发现系统设计}

    微博主题词发现系统总共包括数据获取模块，主题词发现模块，前端展示模块。下面将简单的介绍\\
\\
    \fbox{\begin{minipage}{32em}
    
    1. 数据获取模块\\
    ~~~~1.1. 包括微博登陆模块，爬取模块，存储模块\\
    ~~~~1.2. 使用python语言结合mongodb数据库进行开发\\
    2. 主题词发现模块\\
    ~~~~2.1. 包括数据预处理模块，topic mode模块，pagerank模块\\
    ~~~~2.2. 使用python语言结合gensim开发\\
    3. 前端展示模块\\
    ~~~~3.1. 将主题短语可视化的展示出来\\
    ~~~~3.2. 使用了tornado库结合bootstrap库以及D3.js库开发\\
 \end{minipage}}

本章接下来将依次简略介绍这3个模块。

\subsection{数据获取模块}
    
    数据获取模块，主要包括微博登陆模块，爬取模块，存储模块，主要使用python语言结合MongoDB数据库进行开发。接下来将简单介绍每个模块功能，
\begin{enumerate}
\item 微博登录模块负责模拟登录微博，接下来就可以正常的爬取数据。
\item 爬取模块负责根据关键字进行微博爬取，将爬取数据到的数据存储到数据库。
\item 存储模块负责模拟存储爬取的微博，并且确保存储安全，效率高
\end{enumerate}

    由于存储微博类似于存取文档，因此这里选择使用文档数据库MongoDB，MongoDB是一种文件导向数据库管理系统，MongoDB是一种非结构化数据库，也称为文档型数据库，MongoDB支持key-value的存储模式，这里key可以是我们的搜索关键字，value是基于关键词搜索出来的weibo。

\subsection{主题词发现模块}

    主题词发现模块，主要包括数据预处理模块，主题词发现算法模块，主要使用python语言结合gensim开发。接下来将简单的介绍每个模块的功能，
\begin{enumerate}
\item 数据预处理模块负责清理数据，包括分词，去除停用词等。
\item 主题词发现模块负责使用相关算法计算名词短语权重，并且依据权重排序
\end{enumerate}

    在关主题词发现算法模块中使用了LDA模型，我们利用开源工具包Gensim中的LDA实现，Gemsim是Radim开发的python包，里面有丰富的接口。对于Gensim的LDA的使用如下，
\begin{enumerate}
\item LDA训练使用方法，LDA train usage：Lda = ldaModel(corpus, num\_topics=100)
\item LDA预测使用方法，LDA inference usage：ldaModel.inference(chunk, collect\_sstats=False)
\end{enumerate}

\subsection{前端展示模块}

    前端展示模块需要将主题词可视化，这里使用了python的tornado库结合boostrap库以及D3.js库开发。前端开发使用的是Tornado以及boostrap库，可视化使用的D3.js库。下面将依次介绍。

    Tornado是FriendFeed开发并且由Facebook开源的Web服务框架，它是非阻塞形式的服务器，而且速度相当快。Bootstrap库由Twitter开发并且开源的前端开发框架，具有简洁，直观，强悍的特点，对于我们开发帮助很大。D3.js是由stanford大学开发的开源工具，应用非常广泛，可视化操作简单，是主流的数据可视化工具。D3.js是一个基于数据的操作文档的JavaScript库，可以让你绑定任何数据到DOM，支持DIV这种图案生成，也支持SVG这种图案的生成，D3帮助使用者屏蔽了浏览器差异，因此可以做到代码简洁。


\section{实验设计}
 
    在主题词发现的实验中，我们尝试了2种方法，接下来依次介绍基于TFIDF主题词发现算法，基于LDA和PageRank方法的主题词发现算法的实验设计以及结果。

\subsection{基于TFIDF的主题词发现算法}

    基于TFIDF的主题词发现算法流程，如下，\\
\\
    \fbox{\begin{minipage}{32em}
    1. 新浪微博数据预处理，包括，\\
    ~~~~1.1. 去除重复的微博\\
    ~~~~1.2. 将所有的微博合并到一个文档里面\\
    2. 使用分词工具进行微博分词\\
    ~~~~2.1. 去除停用词以及标点符号\\
    3. 使用正则表达式提取名词短语\\
    ~~~~3.1. 正则表达式是$\left(adjective\right)*\left(nouns\right)+$\\
    4. 对于名词短语根据TFIDF计算公式计算的权重
\end{minipage}}

\subsection{基于LDA以及PageRank的主题词发现算法}
 
    基于LDA以及PageRank的主题词发现算法流程，如下，\\
\\
    \fbox{\begin{minipage}{32em}
    1. 新浪微博数据预处理,与TFIDF的方法一致\\
    2. 使用处理后的微博构建词图,后面将详细介绍词图构建\\
    3. 利用LDA算法获取文档的主题分布以及主题下面单词的概率分布\\
    4. 使用改进后的PageRank算法计算每个主题下面的单词排序\\
    5. 使用词性标注工具对文档进行词性标注\\
    6. 使用正则表达式抽取名词短语，正则表达式跟TFIDF方法一致\\
    7. 依据上文中的名词短语权重计算公式计算最终的权重
    \end{minipage}}
\\
\\
    在算法流程中提到了词图的构建，下面介绍怎么构建词图，首先介绍窗口概念，窗口是以当前词作为源点，以接下里的若干个词作为终点，接着以"I Love Nanjing University very much"中的单词I为例介绍词图。假设窗口为1，那么可以得到边是["I->love"]，窗口为2，可以得到的边是["I->Nanjing"，"I->university"]，窗口为3，可以得到的边是["I->love"，"I->Nanjing"，"I-university"]。在构建基于微博的词图时候，需要将一定数量的微博放在同一个文本里，以此文本为单位构建词图。

\subsection{实验结果分析}

    对比使用TFIDF的模型，基于LDA以及PageRank的主题词发现模型,在F值上取得了提高，同时算法运行时间适中。
\section{总结}

在本章节中，我们在新浪微博语料集上实现了基于LDA和PageRank的主题词发现算法，用LDA模型分解获得隐藏话题信息来丰富特征，使用PageRank算法进行排序，最终我们的算法取得了比基于TFIDF好的效果。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{slot实体在领域分类模型中的使用}\label{chapter_classification}

\section{研究背景}

    随着智能手机的大量使用，语音助手类的应用越来越多的出现在人们的实际生活中，因此通过语音助手产生的数据越来越多，对于语音助手上产生的数据利用以及研究具有实际意义。现在比较流行的语音助手有苹果公司开发的Siri，谷歌公司开发的Google Now，而现在微软也推出一款叫做Cortana的语音助手。Cortana基于人工智能以及大数据的技术，尝试去理解用户的喜好和习惯，具有帮助用户进行日程安排，问题回答等等功能。

    在Cortana的处理逻辑中包括一个称之为领域分类(domain classification)的模块。领域分类模块需要处理的输入有经过语音识别模块处理之后或者通过手写送过来的查询等短文本。领域分类模块负责对输入的查询进行分类，也就是将输入的查询分配到相应的类中去。举例来说，对于查询,“今天苏州下雨了吗”，领域分类模块需要将此查询分类到天气领域，接下来将有其它模块对于分类过的查询进行相应的处理。因此领域分类模块的准确率非常重要，这是接下来对于查询进行一系列更深入处理的入口。微软的语音助手Cortana总共有9个领域，分别是$alarm,places,reminder,calender,weather,note,date,news,web$。因此领域分类模块需要处理的是对于每一个未知的查询输入，将此未知的查询进行分类，分配到上面9个领域中对应的领域中去。

    在领域分类中我们的处理的任务是分类。在机器学习领域，分类被认为是一种有监督学习算法，是使用最普遍的一类算法。分类算法通过对已经标注过类别的语料集分析，通过提取训练语料集的特征建立分类函数，从中发现分类的规则，因而可以用来对未知的数据预测类别。在实际应用中，分类算法使用非常广泛比如金融机构中的风险评估，客户类别的分类，搜索引擎中的文本分类以及软件项目中的漏洞挖掘分类。在分类算法中，广泛使用的有近邻模型(K-Nearest-Neighbor)，朴素贝叶斯模型(Naive Bayes)，支持向量机模型(Support Vector Machine)，神经网络模型(Neural Network)，决策树模型(Decision Tree)等等。分类算法的评价指标有准确率，预测速度，可扩展行以及可解释性等等。
    
    对于微软语音助手Cortana的领域分类处理模块来说，处理的输入是查询或者更确切的说是短文本查询，因此可以将Cortana中的领域分类任务当做是文本分类的一种。对于文本分类来讲，数据的特点是数据规模比较大，特征比较稀疏，支持向量机(SVM)算法经过几十年的使用实践，已经被证明可以在文本分类的领域中有效地使用。因此在领域分类任务中选择使用支持向量机算法。
    
    由于Cortana中的领域分类模块处理的输入查询长度非常短，因此对于每一个查询来讲，可以提取利用的特征数量比较少，基于此想到可以为此查询扩充特征，借助这些扩充的特征来提高领域分类的准确率。实际上在微软的给定的语料中，每一条查询都被人工标上slot，slot是一种领域相关的实体，在后续章节中将会详细介绍slot概念。在领域分类任务中将会对于标注了slot的查询处理，提取相应的特征进行分类，希望提取的结合了slot的特征可以帮助提高领域分类的准确度。实验证明，对每一条标注了slot的查询进行分类确实可以帮助提高准确率。
    
    本章接下来的组织如下，首先我们将概述两种基于slot的分类算法以及两种算法中涉及到的条件随机场模型，支持向量机模型，然后依次详细介绍基于slot的分类算法以及实验设计和结果，最后总结本章。

    
\section{基于slot的分类算法介绍}
   
    首先介绍slot概念，slot一种实体，领域相关，slot具有局部的描述作用，可以用来描述一个单词，不是用来描述整句的\cite{slot}，在上一节研究背景中，我们已经介绍了Cortana共有9个domain。下面以alarm以及weather 领域为例来介绍slot具体概念，在alarm领域slot中包括<alarm\_state>，<set\_alarm>，......，<start\_time>等等。在alarm领域已经标注了slot的一个query例子是"闹钟 设 在 <start\_date> 明 </start\_date> <start\_time> 早 2 点 </start\_time>"，在weather领域中slot包括<data\_range>，<weather\_condition>，......，<suitable\_for>等等。在weather领域中已经标注了slot的一个query例子是"今天 苏州 下雨 <weather\_condition> 了"

    我们使用了两种基于slot的领域分类算法,一种是基于generic slot tagger的domain classification方法，一种是基于in-domain slot tagger的domain classification方法。上面两种方法中的generic slot tagger以及in-domain slot tagger是一种使用条件随机场(CRF)模型训练之后得到的标注模型。
    
    所谓基于generic slot tagger的分类算法，首先对于所有的领域，使用条件随机场模型生成一个slot tagger，然后利用这个唯一的slot tagger对于每一个未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知的query进行分类。

    所谓基于in-domain slot tagger的分类算法，首先对于每一个领域，使用条件随机场模型生成一个slot tagger，然后对于每一个未知的query，分别使用9个in-domain slot tagger对未知的query标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知query进行分类。
    
    微软给定的语料集中每一条query已经标注好slot，为了验证slot特征是否可以提高领域分类的准确率，我们设计了基于人工标注slot的微软数据集的领域分类实验。通过这个实验我们得到了一个oracle(ground truth)结果，准确率是96.8\%。相比于没有slot的分类模型准确率86.3\%结果，有超过10\%的绝对提升，这是非常显著的提高。通过这个实验说明了slot特征可以用于提高领域分类准确率。实际上，对于在线上出现的大量的未知查询，使用人工标注slot是不可行的无论是人工成本或者时间成本都是不可接受的。因此我们希望可以使用一种机器学习算法来帮助实现slot的自动标注，在数据序列化标注中经常用的机器学习算法有隐马尔科夫模型(HMM),条件随机场模型(CRF)以及最大熵马尔科夫模型(MEMM)。基于条件随机场模型在微软使用广泛成熟并且已经效果不错，我们选择条件随机场算法(CRF)来实现slot的标注。
    
    无论对于基于generic slot tagger分类算法或者基于in-domain slot tagger的分类算法来说，处理流程都是首先对数据标注slot，使用条件随机场模型,然后对标注之后的数据进行分类，使用支持向量机模型。因此在接下来的章节中，我们将依次介绍条件随机场模型以及支持向量机模型。

\subsection{条件随机场模型介绍}

    条件随机场(CRF)由Lafferty等人于2001年提出，结合了最大熵模型和隐马尔可夫模型的特点，是一种无向图模型\cite{CRF}，近年来在分词、词性标注和命名实体识别等序列标注任务中取得了很好的效果。条件随机场是一个典型的判别式模型，其联合概率可以写成若干势函数联乘的形式，其中最常用的是线性链条件随机场。
    
    设$X = \left(X_1,X_2,\dots,X_n\right)$和$Y=\left(Y_1,Y_2,\dots,Y_m\right)$是联合随机变量，若随机变量$Y$构成一个无向图$G=\left(V,E\right)$表示的马尔科夫随机场，则条件概率分布$P\left(Y|X\right)$称为条件随机场。条件随机场主要涉及三个问题，一个是概率计算问题，一个是预测算法，一个是参数估计问题。给定条件随机场$P\left(Y|X\right)$，输入序列$x$和输出序列$y$，计算条件概率$P\left(Y_i=y_i|x\right)$,可以使用前向后向算法。条件随机场的预测问题，是给定条件随机场$P\left(Y|X\right)$和输入序列$x$,求条件概率最大的输出序列$y^\ast$,即是对观察序列进行标注，可以使用Viterbi算法完成。我们使用条件随机场模型来对未知的查询标注slot。


\subsection{支持向量机模型介绍}

    SVM算法是一种经典的机器学习算法，是由Boser，Guyon，Vapnik在1992年提出来的，SVM算法的特点是可以同时最小化经验误差与最大化几何间隔\cite{SVM}。从几何意义上来讲SVM算法寻找一个超平面，这个超平面用来分类开正负两类数据点，因此超平面也被成为分离面。基于SVM的优良性质，SVM应用非常广泛，比如文本分类，蛋白质识别，手写数字识别等等。SVM算法在文本分类中的应用非常广泛而且性能非常好。
    
    给定训练集$\left(y_i,x_i\right) \in \{-1,1\} \times R^n,i=1,\dots,l.$ SVM的优化公式如下所示,
    
    $\min\limits_{w} f\left(w\right)$ where $f\left(w\right) \equiv \frac{1}{2} w^\intercal w+C\sum_{i=1}^{l} \xi\left(w;x_i,y_i\right)$。这里的$\xi\left(w;x_i,y_i\right)$是损失函数。

    对大规模的数据进行分类时，需要考虑模型训练时间以及模型大小，在领域分类这个任务上，选择使用$L_1$正则的线性核分类器,$L_1$正则有特征选择作用并且可以得到稀疏解\cite{1normSVM}。接下来解释正则以及线性分类器的概念。在机器学习中，使用正则项是为了进行模型选择，主要目的是避免过拟合，常用的正则项有$L_1$以及$L_2$。我们通过下面的图来解释$L_1$,$L_2$正则项区别
   
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=1.0\textwidth]{L.jpg}\\
      \caption{}\label{fig:test}
    \end{figure}
    
    在图a)中，黑色正方形表示$L_1$正则项的可行解区域，在图b)中，黑色圆形区域代表$L_2$的可行解区域。对于图a)来讲可行解更可能发生在坐标轴，因为图a)跟坐标轴相交。这将意味着在高维空间中，求解系数的大部分维度为0，只有一小部分系数参数保留，从而可以得到稀疏的解。
    \\
    \\
    \\

    在SVM中引入核函数，具体说就是可以将非线性可分的数据扩展到更高维空间使得可以线性可分，这大大的扩展了SVM的能力，使得SVM应用更加广泛\cite{kernelSVM}。然而对于数据规模大，特征数量多并且比较稀疏的语料来说，引入核函数固然可以取得不错的结果，但是却要付出很大的时间代价。有研究表名，在处理这种数据时，使用线性核的SVM可以取得相似的准确率，同时可以极大的减少训练时间。因此对于我们要解决的基于短文本的领域来说，可以使用线性的SVM，处理时间非常快并且可以获得不错的准确率。

    传统上SVM是用来解决二分类的，怎么将SVM用于多分类呢，方法如下，one-vs-rest，one-vs-one等等\cite{MultiSVM}，假设我们有n个类，one-vs-one指的是我们从这n个类中选取两个类$n_i,n_j$出来，然后利用这两个类的数据训练出一个二分类器。以此类推，我们将会有$n^2/2$个分类器。当有测试数据时候，同时使用这么多分类器，然后得到$n^2/2$个分数，选取分值最高所属的类作为测试数据的类。One-vs-rest指的是将其中的一个类作为正例，其余n-1个类作为负例，训练出一个二分类器。依次类推，最后我们将会得到n个二分类器。当遇到一个测试数据时候，同时使用n个二分类器，这样将会得到n个分值，选取分值最高所属的类作为测试数据的类。实际中LibLinear使用的是one-vs-rest方法来做多分类。
    
在我们的后续实验过程中，结合slot作为特征来进行分类，我们可以得到的分类精确度比没有使用slot的分类模型可以提高大概1.9个百分点。

\section{基于generic slot tagger的分类模型}

    所谓基于generic slot tagger的分类算法，首先对于所有的领域，使用条件随机场模型生成一个slot tagger，然后利用这个唯一的slot tagger对于每一个未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知的query进行分类。
    
    本章节组织如下，首先将介绍基于generic slot tagger的分类算法的处理流程,然后是算法运行时间分析，接下来是实验设计及结果。

\subsection{算法流程描述}

    基于generic slot tagger的领域分类的算法描述\\
    \begin{algorithm}
      \caption{classification process based generic slot tagger model}\label{GST}
      \begin{algorithmic}[1]
      \Procedure{Training slot tagger}{}
       \State Training generic slot tagger for all domain
       \State Using generic slot tagger to annotate each query
      \EndProcedure
      \Procedure{Feature Extration}{}
       \State Extracting Features including unigram, bigram and trigram features 
      \EndProcedure
      \Procedure{Classification using SVM}{}
       \State Using SVM model to do domain classification
      \EndProcedure
    \end{algorithmic}
    \end{algorithm}
\\
\\
    基于generic slot tagger的领域分类模型处理流程如下图所示，
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=1.0\textwidth]{generic.jpg}
      \caption{}\label{fig:test1}
    \end{figure}
    
    这种流程处理的好处是对于领域分类任务来说可以灵活的加入更丰富的特征，结构化的信息可以容易的传递给二分类器。

\subsection{算法时间分析}

    从微软给定的语料集来看，Cortana有9个领域，每个领域有大约10种领域相关的slot。所谓领域相关，即是每个领域标注的slot种类不同，举例说明如下，对于alarm领域，slot种类为<alarm\_date>，<set\_alarm>，......，<start\_time>等等。对于weather领域，slot种类为<date\_range>，<weather\_condition>，......，<suitable\_for>。其它领域类似，9个领域slot种类总共是86个。在3.2节这一章节中的实验表明在使用了人工标注slot的微软数据集的分类任务上可以得到的ground truth的准确率为96.8\%。在线上实际运行时，不仅需要考虑分类算法的准确率而且要考虑分类算法的预测时间。接下来将会统计没有使用slot的分类算法的处理时间以及基于generic slot tagger的分类算法的处理时间。

\begin{enumerate}
 \item 假设没有使用slot的分类算法的处理时间为$t_1$，它的处理逻辑是直接使用支持向量机算法对未标注过的未知查询进行分类，假设使用时间是$t_{svm}$，那么$t_1 = t_{svm}$。
 \item 假设基于generic slot tagger的分类算法的处理时间为$t_2$，它的处理逻辑是首先使用generic slot tagger对未知的查询标注slot，假设使用时间是$t_{crf}$，然后使用支持向量机算法对标注过slot的未知查询进行分类，假设使用时间是$t_{svm}$。总的运行时间定义为$t_2 = t_{crf} + t_{svm}$。
\end{enumerate}

    相比于没有使用slot的分类算法，基于generic slot tagger的分类算法时间上将会多出使用条件随机场模型对未知查询标注slot的时间$t_{crf}$。如果$t_{crf}$小于$t_{svm}$，那么我们是可以接受的，然而使用条件随机场模型对未知查询标注slot时，解码的时间复杂度是$O\left(T\times\vert S \vert^2\right)$，这里的$S$指的是slot种类数量，上面提到过Cortana总共有86种slot。使用支持向量机模型对未知查询预测分类的时间复杂度是$O\left(n*m\right)$。基于此使用条件随机场模型对未知查询标注slot时间远大于使用支持向量机模型对未知查询预测的时间，相比于之前没有使用generic slot tagger时候，时间增长太高。即使在上面的实验中使用人工标注slot的微软数据集的分类算法可以得到的ground truth的准确率为96.8\%,从时间上考虑，对于线上应用也是不能接受的。

\subsection{模拟实验}

    为了满足线上时间需求，我们想到的是减少slot种类，从而可以减少generic slot tagger的解码时间。我们的方法是从86种slot中选取少量具有代表性的19种slot，接下来需要验证这19种slot是否可以提高领域分类的准确率。基于此，我们设计了下面的实验，对于语料集做了相应处理，即是对于所有标注slot的查询，只保留19种slot，其余的全部移除。然后在处理后的语料集中进行分类算法的实验，可以得到的ground truth准确率是91\%，相对于没有使用slot的模型准确率86.3\%来讲这也是一个非常显著的提升。
    
    接下来关注基于微软语料集训练的条件随机场模型在实际中使用的效果。为了了解条件随机场模型的F值也即slot标注效果对于领域分类的影响。我们做了一个在微软语料集上模拟slot标注的实验，模拟序列化标注的行为，我们随机的对于每一条查询的slot进行增加，删除，改变三个行为。具体的来说就是对于每一个查询的每个词，当此词没有slot时，以一定的概率给定一个slot，当此词有slot时，以一定的概率选择删除slot或者使用其它slot替换此slot，经过如此处理之后计算模拟标注slot的F值。基于模拟实验，可以生成虚拟的F值范围为40\%-60\%之间标注模型。最后在处理过的语料集上进行分类算法实验，观察模拟实验得到的标注模型的不同的F值与准确率的关系，下图是我们的模拟实验\\
\\
\\
\\
    
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=1.2\textwidth]{simi.jpg}
      \caption{}\label{fig:test1}
    \end{figure}

    
    从图中曲线可以观察得知F值在60\%及以上，分类算法的准确率有提高。这意味着如果可以训练得到一个F值在60\%及以上的条件随机场模型，分类模型的准确率可以得到提高。接下来是我们的实验设计。

\subsection{实验设计}

    我们使用支持向量机模型来做领域分类任务。使用微软给定的语料集来实验，其中有400万查询作为训练数据，1.8万查询作为测试数据。我们使用LIBLINEAR\cite{LibLinear}来训练支持向量机模型，LIBLINEAR是台湾大学林智仁组开发的一种线性分类器，可以用来处理具有数以百万计实例和特征的数据。LIBLINEAR支持L2正则L2损失的线性SVM分类器，L2正则L1损失的线性SVM分类器，L1正则L2损失的线性SVM分类器等等。LIBLINEAR跟LIBSVM具有很多相似地方，比如跟LIBSVM处理数据格式一致，使用方法类似。对于多分类算法，LIBLINEAR支持使用one-vs-rest，Crammer\&Singer等等策略。同时LIBLINEAR接口丰富，支持MATLAB/Octave，Java，Python以及Ruby等等。LIBLINEAR使用分为训练过程以及预测过程如下,\\
\\
    \fbox{\begin{minipage}{32em}
    1. 训练过程的使用方式Usage: train [options] training\_set\_file [model\_file]\\
    ~~~~1.1. train是LIBLINEAR提供的训练工具\\
    ~~~~1.2. training\_set指的是训练语料集\\
    ~~~~1.3. model\_file指的是训练之后得到的模型文件\\
    2. 预测过程的使用方式Usage: predict [options] test\_file model\_file output\_file\\
    ~~~~2.1. predict是LIBLINEAR提供的预测工具\\
    ~~~~2.2. test\_file指的是测试语料集\\
    ~~~~2.3. model\_file是在训练过程中产生的模型，用于在测试语料集上预测\\
    ~~~~2.4. output\_file是trai测试语料test\_file上的预测输出\\
    \end{minipage}}
\\
\\  
     为了防止随机性对于实验结果的影响，我们重复实验50次，取准确度的平均值作为最终的准确率。实验设计流程如下，\\
\\
    \fbox{\begin{minipage}{32em}
    1. 对于给定语料进行数据预处理，处理过程如下，\\
    ~~~~1.1. 使用微软内部分词工具分词\\
    ~~~~1.2. 去除停用词\\
    2. 特征提取，处理过程如下,\\
    ~~~~2.1. 特征集合是在训练集上提取\\
    ~~~~2.2. 对于测试集来讲，所有不在特征集合的特征都被舍弃掉\\
    ~~~~2.3. 在微软给定的语料集上得到的特征数量是875万\\
    3. 对每一条查询的提取特征进行归一化,归一化的好处如下，\\
    ~~~~3.1. 可以防止某一维或某几维对数据影响过大\\
    ~~~~3.2. 可以使程序可以运行更快\\   
   4. 将特征处理成支持向量机使用的格式\\
   ~~~~4.1. 在处理好的训练集中使用LIBLINEAR训练支持向量机模型\\
   ~~~~4.2. 在训练集上训练支持向量机模型\\
   ~~~~4.3. 在测试集上测试支持向量机模型，记录模型的准确率\\
   \end{minipage}}
\\
\\
    基于以上实验流程，我们分别实现了non-slot的模型以及基于generic slot tagger的模型。这两种模型的区别仅仅是使用的特征不同，对于non-slot模型来讲，我们仅仅使用了文本特征，对于基于generic slot tagger的模型来讲，我们将query中的slot看成跟词一样的项，在特征提取过程中，提取了三种类型的特征。在特征提取的策略上，对于两种模型都是提取了ngram的特征，不同的地方是基于generic slot tagger的模型利用了slot实体。我们以查询"闹铃 设 在 <start\_date> 明 </start\_date> <start\_time> 早 2 点 </start\_time>"为例，讲述两种模型的特征提取方法。
    对于没有slot的分类模型的特征提取，我们提取了文本的unigram，bigram以及trigram特征，具体如下，\\
\\
    \fbox{\begin{minipage}{32em}
    1. unigram特征["闹铃","设"，"在","明","早","2","点"]\\
    2. bigram特征["闹铃 设","设 在","在 明","明 早","早 2","2 点"]\\
    3. trigram特征["闹铃 设 在","设 在 明","在 明 早",，...]\\
    \end{minipage}}
\clearpage
    对于基于generic slot tagger模型的特征提取，首先提取查询本身的ngram特征，其次提取slot集合的ngram特征，最后提取查询跟slot一起的ngram特征。具体如下表示,\\
\\
    \fbox{\begin{minipage}{32em} 
    1. 基于查询本身的特征\\
    ~~~~1.1. unigram特征["闹铃","设"，"在","明","早","2","点"]\\
    ~~~~1.2. bigram特征["闹铃 设","设 在","在 明","明 早","早 2","2 点"]\\
    ~~~~1.3. trigram特征["闹铃 设 在","设 在 明","在 明 早,，...]\\
  
    2. 基于slot本身的特征
    
    基于上述query提取的slot集合是"<start\_date> </start\_date> <start\_time> </start\_time>",对此slot集合提取unigram，bigram，trigram特征
    
    ~~~~2.1. unigram特征是["<start\_date>", "</start\_date>","<start\_time>", "</start\_time>~~"]\\
    ~~~~2.2. bigram特征是["<start\_date> </start\_date>","</start\_date> <start\_time>", "<start\_time> </start\_time>"]\\
    ~~~~2.3. trigram特征是["<start\_date> </start\_date> <start\_time>", "</start\_date> <start\_time> </start\_time>"]\\

    3. 基于查询以及slot的特征
   
    在上述的查询中，我们将slot跟单词同样对待，对此查询提取unigram，bigram，trigram特征。

    ~~~~3.1. unigram特征是["闹铃"，"设"，"在"，"<start\_date>"，"明"，"</start\_date>"，"<start\_time>"，"早"，"2"，"点"，"</start\_time>"]\\
    ~~~~3.2. bigram特征是["闹铃 设"，"设 在"，"在 <start\_date>"，"<start\_date> 明"，"明 </start\_date>"，"<start\_date> <start\_time>",]\\
    ~~~~3.2. trigram特征是["闹铃 设 在", "设 在 <start\_date>", "在 <start\_date> 明",...]\\
\end{minipage}}
 
\subsection{实验结果分析}

    对于没有使用slot的分类模型，我们取得了86.3\%的准确率。对于基于generic slot tagger的分类模型，实际上在使用F值为60\%至80\% CRF模型对query进行标注，然后在标注之后上的语料进行分类算法的实验时得到的分类的准确率是86.1\%,并没有相应提高。这是一个需要未来解决的问题。由于基于generic slot tagger的分类模型没有取得预期的提高，接下来我们使用了基于in-domain slot tagger的分类模型，并且取得了准确率上的提高,我们将在下一章中进行介绍。

\section{基于in-domain slot tagger的分类模型}

    在上一章介绍过，在微软给定的语料集中，总共有9个领域，每个领域大约有10种slot。每个领域的slot种类跟其它领域之间几乎没有重合的，也即slot是领域相关的。

    所谓基于in-domain slot tagger的分类算法，首先对于每一个领域，使用条件随机场模型生成一个slot tagger，然后对于每一个未知的查询，分别使用9个in-domain slot tagger对未知的查询标注slot实体。接着使用支持向量机(SVM)算法，在已经标注了slot的语料集上训练分类器得到分类模型，最后使用得到的分类模型对于未知查询进行分类。
    
    本章节组织如下，首先将介绍基于in-domain slot tagger的分类算法的处理流程,然后是算法时间分析，接下来是实验设计及结果。

\subsection{算法流程描述}

    在上面的介绍中，我们了解到Cortana有9个领域。在接下来的算法描述中，涉及到in-domain以及anti-domain概念，in-domain指的是9个领域中的任意一个领域，anti-domain指的是除了in-domain之外的其余所有领域。举例说明，假设alarm是in-domain，那么anti-domain指的是reminder，calender，palces，weather，web，date，news，note。那么in-domain slot tagger指的使用任意一个领域语料集训练出来的条件随机场模型。

    基于in-domain slot tagger的领域分类流程描述
    \begin{algorithm}
      \caption{classification process based in-domain slot tagger model}\label{IDST}
      \begin{algorithmic}[1]
      \Procedure{Training Slot Tagger}{}
       \State Training slot tagger for each domain
      \EndProcedure
      \Procedure{Annotating slot}{}
       \State Using in-domain tagger to annotate anti-domain query, then
       \State Treating in-domain querys as positive examples
       \State Treating anti-domain querys as negative examples
      \EndProcedure
      \Procedure{Training binary classification model}{}
       \State For each in-domain querys, treating as positive examples
       \State For anti-domain querys, treating as negative examples.
       \State Training binary classifier for each in-domain
      \EndProcedure
      \Procedure{Using Slot tagger and SVM model for domain classification}{}
       \State Using each in-domain slot tagger to annotate each unknown query, then
       \State Using each in-domain binary classifier to rate annotated query, then
       \State Comparing all socres and attach highest score's domain to unkown query
      \EndProcedure
      \end{algorithmic}
   \end{algorithm}

\subsection{算法时间分析}
    
    与基于generic slot tagger的分类算法一样，需要考虑线上运行时间，接下来将会统计没有使用slot的分类算法的处理时间以及基于in-domain slot tagger的分类算法的处理时间，具体如下：
   \begin{enumerate}
     \item 假设没有使用slot的分类算法的处理时间为$t_1$，它的处理逻辑是直接使用SVM算法对标注过的未知查询进行分类，假设使用时间是$t_{svm}$，那么$t_1 = t_{svm}$。
     \item 假设基于in-domain slot tagger的分类算法的处理时间为$t_2$，它的处理逻辑是首先使用每一个in-domain的slot tagger对未知的查询标注slot，对于9个领域来讲这个过程是可以并行执行的，假设使用时间是$t_{crf}=\max\limits_{i \in I} t_{{crf}_i},I \in (0,9)$ ，然后使用支持向量机算法对标注过slot的未知未知进行分类，假设使用时间是$t_{svm}$。总的运行时间定义为$t2 = t_{crf} + t_{svm}$。
   \end{enumerate}

    因为每个domain中slot种类数量不多，每个domain不仅可以容易的获取F值很高的条件随机场模型。同时对于每个domain的条件随机场模型解码时间在可以接受的范围内。
    
    接下来我们将会做一组实验，对比没有使用slot的分类模型与基于in-domain slot tagger的分类模型准确率。

\subsection{实验设计}
    
    基于in-domain slot tagger的分类算法与基于generic slot tagger的分类算法数据处理，特征提取等方式是一致的。不同在于，CRF模型生成方式，分类器训练过程以及分类预测过程。接下来我们将简述不同之处。\\
\\
    \fbox{\begin{minipage}{32em}
    1. 对于每一个领域（domain），训练出一个CRF模型，也即生成一个slot tagger\\
    2. 对于每一个领域训练出一个二元分类器，训练过程如下\\
    ~~~~2.1  使用当前领域的slot tagger去标注其它领域(除了当前领域的其它所有领域)的查询，作为负例\\
    ~~~~2.2. 使用当前领域的query作为正例\\
    ~~~~2.3. 使用SVM模型训练二元分类器， 最终生成9个二元分类器\\
    3. 分类预测过程\\
    ~~~~3.1. 当遇到未知查询时，使用每个领域的slot tagger对此未知查询标注slot，接着，\\
    ~~~~3.2  使用每个领域的二元分类器对已经标注过slot的查询进行分类打分，最终生成9个分数，接着，\\
    ~~~~3.2. 对比所有的分数，将分数最高对应的类赋予未知的查询\\
    \end{minipage}}

\subsection{实验结果分析}

    对于没有使用slot的模型，我们取得了\large{\textbf{86.3\%}}的准确率。对于基于in-domain slot tagger的分类模型,我们取得了\large{\textbf{88.2\%}}的准确率。
    综上，基于in-domain slot tagger的分类模型在准确率上得到了提高同时运行时间在可以接受的范围内，所以这个算法可以在线上运行。

\section{总结}

    通过我们的模拟实验可以得知slot实体对于分类是有提高作用的，然而对于如何使用slot确有不同的方法。我们在实验过程中尝试了基于generic slot tagger的分类模型以及基于in-domain slot tagger的分类模型。相比于没有使用slot的分类模型，基于generic slot tagger的分类模型没有在准确率上没有达到预期，然而基于in-domain的slot tagger的分类模型可以提高分类的准确率，并且由于基于in-domain slot tagger的分类模型标注slot需要的时间很短，可以满足线上的使用。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{增量式学习在领域查询分类中的使用}\label{chapter_incremental}

\section{研究背景}

   随着智能手机的普及，使用智能手机上的语音助手的人越来越多，因此从语音助手中可以获取的数据量越来越大。这对于公司的信息挖掘以及知识获取是一种挑战，随着信息的更新速度越来越惊人，公司可以获取的数据量将会越来越丰富。对于微软的语音助手Cortana来说,每周都可以从Cortana的对话记录中得到大规模数据。在上一章中我们提到了使用基于Cortana对话语料集来实现查询分类的任务。在这个任务中，使用支持向量机(SVM)算法用来分类，对于机器学习算法来说，泛化能力可以用来衡量机器学习算法的效果。所谓泛化能力指的是机器学习算法对于未知数据的预测能力，具体到查询分类中使用的支持向量机(SVM)算法，泛化能力指的是使用支持向量机(SVM)算法训练出来的模型对于未知查询预测效果。基于此，我们希望经过支持向量机训练出来的分类模型具有更强的泛化能力。
   
   模型的泛化能力受两方面的影响，一方面是模型的复杂度，泛化能力跟模型的复杂度成负相关，另一方面是数据的规模，泛化能力跟数据规模成正相关。大规模的数据对于提升模型的泛化能力很重要。这意味着如果我们可以将Cortana更新之后的数据全部利用起来训练新的模型，那么就可以帮助提升模型的泛化能力，具体到领域查询分类来讲就是可以提升使用支持向量机训练的分类模型对于未知查询的分类准确度。
   
   利用大规模的数据训练算法模型的同时也伴随着一个不能忽视的问题，随着可以利用的数据越来越多，重零开始训练模型需要的时间越来越长。对于分类任务来说即将面临的问题是一方面大规模的数据可以帮助训练出泛化能力更强的模型，另一方面如果每次都是重新开始训练模型，训练时间将会大大的增加，这对于在线上使用是不可以接受的。而实际情况是，我们已经在原始语料集上训练出了效果很好的模型，与此同时每周都会有大规模数据到来。为了提高分类模型的精确度，最朴素的做法是选择使用更新后的语料集来重新训练支持向量机(SVM)模型，但是这将付出很大的时间代价。所以我们需要某种算法，可以取得跟重新开始训练的模型一致的准确度并且大大的减少训练时间，至少可以满足现在的训练时间上的需求。在[C.J.Lin]\cite{incremental}这篇论文里提出了一种增量式学习(incremental learning)的算法，基本思想可以概述为训练新的分类模型时，一方面使用更新后的数据进行训练，另一方面利用之前使用原始数据训练出来的模型参数初始化新模型的参数，以期新的模型既可以取得跟使用更新之后的全部数据重新训练的模型一致的准确度又可以减少训练时间。我们在微软给定的数据集中应用了这个算法，并且在微软的给定的语料集上重现出实验效果。

   本章接下来的组织如下，首先我们将概述基于支持向量机的增量式学习算法与SVM原始问题和对偶问题的联系，通过比较我们决定求解支持向量机中基于原始问题的增量式学习问题。然后介绍基于原始问题的增量式学习算法求解中涉及到的坐标梯度下降算法以及置信域的牛顿算法的时间效率，接着阐述实验设计以及结果，最后进行总结。

\section{基于SVM的增量式学习算法}

    我们求解的是基于SVM的增量式学习算法\cite{incremental}，在第二章我们已经详细介绍过SVM，由于SVM的优化公式的形式属于凸优化范畴，而对于凸函数来说存在着全局最优解，这意味着可以使用很多高效的优化算法来求解SVM算法的参数。需要特别指出的是增量式学习算法中使用的SVM是L2正则L2损失函数形式。SVM的求解算法可以分为基于原始问题的求解算法以及基于对偶问题的求解算法。

    接下来将直接介绍SVM原始问题的形式及其求解方法，对偶问题的形式及其求解方法以及这两种问题的求解方法与增量式学习的联系。

\subsection{SVM原始问题介绍}

    给定训练集$\left(y_i,x_i\right) \in \{-1, 1\} \times R^n, i = 1,\dots,l$,SVM原始问题的优化公式是

    $\min\limits_{w} f\left(w\right)$, where $f\left(w\right)\equiv \frac{1}{2}w^Tw + C\sum_{i=1}^l \xi\left(w;x_i;y_i\right)$, 

    $\xi\left(w;x_i,y_i\right)$是损失函数，这里使用的是L2损失函数，也即$\xi_L2\left(w;x_i,y_i\right) \equiv \max\left(0,1-y_iw^Tx_i\right)^2$。L2损失函数的特点是可微但不是二次可微。存在很多凸优化算法寻找最优的参数$w$。

   在[Z.Liang and Y.Li.]\cite{Primal2009}论文中，详细描述了当数据发生变化时候，求解基于原始问题的增量式学习算法。
  
   在原始问题中，SVM的参数w的维度跟数据变化无关，跟特征数量一致。当训练数据增加时候，对于增量式学习算法有以下两个假设：第一个假设是，特征数量不发生变化，因此对于原始问题需要求解的参数数量也没有发生变化；第二个假设是，新增加的数据跟原始数据服从于同一分布。因为求解原始问题的优化算法是一种迭代算法，因此好的初始值可以减少迭代次数从而减少训练时间。对于原始的训练集$\left(y_i,x_i\right),i=1,\dots,l$，假设$w^*$是原始问题的最优解，基于以上假设当增加新的数据时，可以利用$w^*$来作为新的模型参数的初始值也即$\bar{w}=w^*$。如果采用朴素的训练算法，新数据增加后的优化公式是

    $\min\limits_{w} \frac{1}{2}w^Tw+C\sum_{i=1}^{l+k} \xi\left(w;x_i,y_i\right)$，

    如果使用了原始数据训练出来的模型参数$w^\ast$作为新的优化问题的初始化参数，那么新的问题的初始值是

    $\frac{1}{2}\bar{w}^T\bar{w}+C\sum_{i=1}^l
\xi\left(\bar{w};x_i,y_i\right)+C\sum_{i=l+1}^{l+k}\xi\left(\bar{w};x_i,y_i\right)$。

\subsection{SVM对偶问题介绍}
    
    给定训练集$\left(y_i,x_i\right) \in \{-1,1\} \times R^n$，最优化参数$w$可以表示为带有系数$\alpha$的训练样本的线性组合，也即$w=\sum_{i=1}^l y_i\alpha_ix_i$。我们可以解决基于$\alpha$的优化问题，也就是SVM问题的对偶形式，

    $\max\limits_{\alpha} f^D\left(\alpha\right) \equiv \sum_{i=1}^l h\left(\alpha_i,C\right)-\frac{1}{2}\sum_{i=1}^l \sum_{j=1}^l \alpha_i\alpha_j K\left(i,j\right)-\sum_{i=1}^l \frac{\alpha_i^2}{2}d$,$0 \le \alpha_i \le U$,$\forall i=1,\dots,l$，

    这里的$K\left(i,j\right)=y_iy_jx^T_ix_j$,同时对于L2损失的SVM来讲有，$U=\infty$,$d=\frac{1}{2C}$,$h\left(\alpha_i,C\right)=\alpha_i$。
    
    在[G.Cauwenberghs]\cite{NIPS20011},[S.Fine]\cite{NIPS20012},[M.Karasuyama]\cite{TNN2010},[P.Laskov]\cite{JMLR2006}等论文中，详细描述了当数据变化时候，求解基于对偶问题的增量式学习算法。
    
    对于对偶问题，求解的参数是$\alpha$,$\alpha$的维度跟数据维度一致，因此当增加新的数据时，$\alpha$的维度会发生变化。这里假设增加的新的训练集如下$(y_i,x_i),i = l+1,\dots,l+k$,下面的$\bar{\alpha}$可以视为初始的解，$\bar{\alpha} = [\alpha^\ast_1,\dots,\alpha^\ast_l,0,\dots,0]^\intercal \in R^{\left(l+k\right) \times l}$对于$\bar{\alpha_{l+1}},\dots,\bar{\alpha_{l+k}}$可以在$[0,U]$之间任意取值，如何最优地对其进行赋值是求解incremtal learning中的对偶问题面临的问题。

    新数据进来后优化公式是，
    
    $\min\limits_{w} \frac{1}{2}w^\intercal w+C\sum_{i=1}^{l+k} \xi\left(w;x_i,y_i\right)$，

    假设我们对$\bar{\alpha_{l+1}},\dots,\bar{\alpha{l+k}}$取值为0，那么增加新的数据之后优化公式的初始值是，
    
    $\sum_{i=1}^l h\left(\alpha^\ast_i,C\right)+\sum_{i=l+1}^{l+k} h\left(0,C\right)-\frac{1}{2}[{\alpha^\ast}^\intercal O^\intercal]\left[{\begin{array}{ccccc} \bar{Q} & \vdots \\ \dots & \\ \end{array}}\right] = \frac{1}{2}\bar{w}^\intercal \bar{w}+C\sum_{i=1}^l \xi\left(\bar{w};x_i,y_i\right)$
    
\subsection{SVM解法总结}

    在论文"Incremental and decremental learning for linear classification"中有证明，原始问题初始值相对于对偶问题初始值距离新的问题的最优值更近，这意味着求解原始问题的时间比较少。同时对偶问题的初始值不好确定，这意味着在工程上原始问题更容易实现。综上，在求解基于SVM的增量式学习算法问题时候选择求解svm原始问题。

\section{增量式学习中的优化算法}

优化算法是一种应用数学方法，在机器学习中使用非常广泛，下面我们简单介绍最优化方法形式以及一般的求解方法.最优化方法主要研究以下形式的问题：给定一个函数$f:A\rightarrow\Re$，寻找一个元素$X^0 \in A$，使得对于所有$A$中的$x$，$f\left(x^0\right) \le f\left(x\right)$(最小化);或者$f\left(x^0\right) \ge f\left(x\right)$(最大化)。很多优化问题都可以建模成这样的一般性形式。一般情况下，会存在若干个局部的极小值或者极大值。局部极小值$x^\ast$定义为对于一些$\delta>0$，以及所有的$x$满足$\Vert x-x^\ast\Vert \le \delta$,公式$f\left(x^\ast \right) \le f\left(x\right)$成立。这就是说，在$X^\ast$周围的一些闭球上，所有的函数值都大于或者等于在该点的函数值。一般的，求局部极小值是容易的，但是要确保其为全域性的最小值则需要一些附加性的条件，例如，该函数必须是凸函数。
    
    对于优化算法，经常使用的有两种，一种是一阶优化算法，这种算法的优点是每次迭代代价比较小，具体来说就是求解一阶导数的代价较低，缺点是迭代次数比较多。另一种是二阶优化算法，这种算法的优点是迭代次数比较少，因为这里求解的是二阶导数，是梯度的梯度，缺点每次迭代的计算代价比较大，具体来说要求Hessian矩阵的逆矩阵。
    
    由于需要求解的优化公式具有非常好的性质，具体的说求解的优化函数是一种凸函数，具有全局最优解，尤其需要注意的是求解的SVM是L2正则L2损失函数，可以容易的求得优化函数的一阶导数以及二阶导数，因此可以使用一系列高效的优化算法。这里选择两种求解SVM问题比较高效的优化算法，通过比较两种优化算法的时间开销并且关注两种优化算法求解的SVM模型在测试集上的准确率，最后确定一种比较好的的优化算法。这两种优化算法是论文"Coordinate Descent Method for Large-scale L2 loss Linear SVM"中提到的坐标梯度下降优化算法和论文"Trust Region Newton Method for Large-scale Logistic Regression"提到的置信域牛顿优化算法。

\subsection{坐标梯度下降优化算法}
    
    坐标梯度下降算法在每次迭代时，在当前点处沿一个坐标方向进行一维搜索以求得一个函数的局部极小值，在整个过程中循环使用不同的坐标方向。

    在论文"Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines"\cite{CDM}中提到的坐标梯度下降优化算法可以用来解决SVM的原始问题，并且相当高效。增量式学习算法中求解是L2正则L2损失的SVM形式如下，

    $\min\limits_{w} f\left(w\right) = \frac{1}{2}w^\intercal w+ C\sum_{j=1}^l \max\left(1-y_jw^\intercal x_j,0\right)^2$
    
    L2损失SVM是分段二次凸函数，一阶可微但不是二阶可微的。坐标梯度下降优化算法是一种迭代算法，通过每一轮对参数$w$进行迭代计算，产生序列$\{w^k\}_{k=0}^{\infty}$，直至参数收敛。
    
    坐标梯度下降算法每次通过解决子问题来更新参数$w$的一个变量，如果可以高效的解决这个子问题那么坐标梯度算法将会很有竞争力。对于L2损失的SVM，子问题是最小化单变量的分段二次函数，这个函数一阶可微但是二阶不可微。子问题可以形式化表示为     

    $w_i \leftarrow w_i + \arg\min_{d} f\left(w+de_i\right)$。
    
    坐标梯度下降算法中使用了牛顿方法来近似的求解上述子问题并且在论文中证明基于坐标梯度下降求解SVM算法的时间复杂度是$O\left(nm\right)$[5]

    坐标梯度下降算法如下，
    \begin{algorithm}
    \caption{Coordinate descent algorithm for L2-SVM}\label{CD}
    \begin{algorithmic}[1]
    \Procedure{Coordinate descent procedure description}{}
      \State Start with any initial $w^0$
      \For{$k = 0,1,\dots$}
        \For{$i = 1,2,\dots,n$}
	    \State Fix $w^{k+1}_{1},\dots,w^{k+1}_{i-1},\dots,w^{k}_{n}$,approximately solve the sub-problem to obtain $w^{k+1}_{i}$
          \Procedure{sub-problem procedure}{}
	      \State Given $w^{k,i}$. Choose$\beta \in \left(0,1\right) \left(e.g.,\beta=0.5\right)$.
	      \State Calculate the Newton direction $d=-D^{'}_{i}\left(0\right)/D^{''}_{i}\left(0\right)$
	      \State Compute $\lambda = \max\{1,\beta,\beta^2,\dots\}$ such that $z = \lambda d$ statisfies $D_{i}\left(z\right) - D_{i}\left(0\right) \le -\sigma z^2$
          \EndProcedure
        \EndFor
      \EndFor
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}

    $D^{'}_i\left(z\right) = w^{k,i}_i + z -2C \sum_{j \in I\left(w^{k,i}+ze_i\right)} y_jx_{ji}\left(b_j\left(w^{k,i}+ze_i\right)\right)$
    
    $D^{''}_i = 1 + 2C\sum_{j \in I\left(w^{k,i}+ze_i\right) x^{2}_{ji}}$
\\
\\
\\
\\
\\
    
\subsection{置信域牛顿优化算法}
   
    
    置信域牛顿优化算法解决的SVM问题与坐标梯度优化算法一致。对于牛顿方法来说，每一轮迭代的代价高同时具有很快的收敛速度。

    置信域牛顿方法也是一种迭代算法，在每一轮迭代中对于参数$w$，置信域牛顿优化算法通过获得近似的牛顿步长$d$来获取置信域通过解决下面的子问题\cite{TRON},

    $\min_{d} q\left(d\right)$ subject to $\Vert d \Vert \le \Delta$， where $q\left(d\right) \equiv \Delta f\left(w\right)^\intercal+\frac{1}{2} d^\intercal \Delta^2 f\left(w\right)d$。
    
    这个子问题是通过共轭梯度下降进行求解的。置信域牛顿算法过程如下，
    
    \begin{algorithm}
    \caption{Trust region algorithm for L2-SVM}\label{TRON}
    \begin{algorithmic}[1]
    \Procedure{Trust region algorithm procedure description}{}
      \State Start with any initial $w^0$
       \For{$k = 0,1,\dots$}
         \If{$\Delta f\left(w^k\right)=0$},stop
           \State Find an approximate solution $s^k$ of the trust region sub-problem
           \State $\min\limits_{s} q_k\left(s\right)$,subject to $\Vert s \Vert \le \Delta_k$
           \State Compute $\rho_k$ via $\left(1\right)$
           \State Update$w_k$ to $w_{k+1}$ according to $\left(2\right)$
           \State Obtain $\Delta_{k+1}$ according to $\left(3\right)$
         \EndIf
       \EndFor
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
  
    $\left(1\right)$  $\rho_k = \frac{f\left(w^k + s^k\right)-f\left(w^k\right)}{q_k\left(s^k\right)}$
   
    $\left(2\right)$  $w^{k+1} =  \begin{cases} 
      w^k+s^k & if~~\rho_k > \eta \\
      w^k & if~~ \rho_k \le \eta
   \end{cases}$

    $\left(3\right)$ $\Delta_{k+1} \in [\sigma_1\min\{\Vert s^k \Vert,\Delta_k]~~~~if~~ \rho \le \eta_1,$

    ~~~~~~$\Delta_{k+1} \in [\sigma_1\Delta_k,\sigma_3\Delta_k]~~~~~~~~~if~~\rho_k \in \left(\eta_1,\eta_2\right)$

    ~~~~~~$\Delta_{k+1} \in [\Delta_k,\sigma_3\Delta_k]~~~~~~~~~~~~~if~~\rho_k \ge \eta_2$
    
    在求解置信域的牛顿算法过程中需要求解子问题，子问题可以通过共轭梯度算法求解，共轭梯度下降算法描述如下，
    
    \begin{algorithm}
    \caption{Conjugate gradient procedure for approximately solving the trust region sub-problem}\label{CG}
    \begin{algorithmic}[1]
    \Procedure{Conjugate gradient algorithm procedure description}{}
      \State Given $\xi_k < 1,\Delta_k > 0. Let \bar{s}^0 = 0,r^0 = -\Delta f\left(w^k\right) and d^0 = r^0$
       \For{$i = 0,1,\dots$}
         \If{$\Vert r^i \Vert = \Vert \Delta f\left(w^k\right) + \Delta^2 f\left(w^k\right) \bar{s}^i \le \xi_k \Vert \Delta f\left(w^k\right) \Vert$}
           \State output $s^k = \bar{s}^i $ and stop
         \EndIf
         \State $\alpha_i = \Vert r^i \Vert^2/\left(\left(d^i\right)^\intercal \Delta^2 f\left(w^k\right) d^i\right)$
         \State $\bar{s}^{i+1} = \bar{s}^{i} + \alpha_i d^i$
         \If $\Vert \bar{s}^{i+1} \Vert \ge \Delta_k$,compute $\tau$ such that
           \State $\Vert \bar{s}^i + \tau d^i \Vert = \Delta_k$
           \State output$s^k = \bar{s}^i + \tau d^i$ and stop
         \EndIf
         \State $r^{i+1} = r^{i} - \alpha_i \Delta^2 f\left(w^k\right) d^i$
         \State $\beta_i = \Vert r^{i+1} \Vert^2 / \Vert r^i \Vert^2$
         \State $d^{i+1} = r^{i+1} + \beta_i d^i$
       \EndFor
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}

\section{实验设计以及结果分析}


    实验使用微软给定的语料集，其中400万条查询作为训练语料集，1.8万条查询作为测试语料集。为了防止随机性对于实验结果的影响重复实验50次，取平均的训练时间以及准确率作为做最终的结果。

    每次实验中，训练集按照如下方法获取，首先将训练语料集进行洗牌处理，然后将数据划分为10份，也即第一份数据包含原始数据的$\frac{1}{10}$,第二份数据包含原始数据的$\frac{2}{10}$,$\dots$,最后一份数据包含$\frac{10}{10}$。每次实验有十份不同的训练集，测试集依然是原始的测试语料集。这里称每次都是重新训练的方法为non-incremental learning训练方法，每次训练使用以前模型参数初始化的方法为incremental learning训练方法。
    
    接下来将分别阐述non-incremental learning训练方法和incremental learning训练方法以及具体的实验设计。

\subsection{non-incremental learning训练流程}

    对于non-incremental learning训练方法，分别使用原始数据经过洗牌处理之后的$\frac{1}{10},\frac{2}{10},\frac{3}{10},\dots,\frac{10}{10}$的数据作为训练集，训练出分类器$X_1,X_2,X_3,\dots,X_{10}$。以第一个，第二个以及第十个分类器为例说明训练方法。\\
\\
    \fbox{\begin{minipage}{32em}
     1. 第一个分类器，使用$\frac{1}{10}$数据作为训练集，重新开始训练出分类器$X_1$。记录$X_1$的训练时间以及在测试集上的准确率。\\
     2. 第二个分类器，使用$\frac{2}{10}$数据作为训练集，重新开始训练出分类器$X_2$。记录$X_2$的训练时间以及在测试集上的准确率。\\
     3. $\dots \dots$\\
     4. 第十个分类器，使用完整的语料集作为训练集，重新开始训练出分类器$X_10$。记录$X_10$的训练时间以及在测试集上的准确率。
    \end{minipage}}

\subsection{incremental learning训练流程}
    
    对于incremental learning方法，分别使用原始数据洗牌之后的$\frac{1}{10},\frac{2}{10},\frac{3}{10},\dots,\frac{10}{10}$的数据作为训练集训练分类器，不同的是训练方法。以第一个，第二个，第十个分类器为例说明训练方法。
\\
\\
   \fbox{\begin{minipage}{32em}
    1. 第一个分类器，使用$\frac{1}{10}$的数据作为训练集，训练出分类器$Y_1$。记录$Y_1$在的训练时间以及在测试集上的准确率\\
    2. 第二个分类器，使用$\frac{2}{10}$的数据作为训练集同时使用分类器$Y_1$的参数作为第二个分类器参数的初始值，训练出分类器$Y_2$。记录$Y_2$的训练时间以及在测试集上的准确率\\
    3. $\dots \dots$\\
    4. 第十个分类器，使用完整的语料集作为训练集同时使用分类器$Y_9$的参数作为第十个分类器参数的初始值，训练出分类器$Y_{10}$。记录$Y_{10}$的训练时间以及在测试集上的准确率
    \end{minipage}}

\subsection{实验设计}
    
    对于non-incremental learning方法和incremental learning方法中的每一个分类器，以及每一个分类器中的训练集以及测试集的处理方法一致，具体如下\\
\\  
   \fbox{\begin{minipage}{32em}
    1. 训练集以及测试集进行数据预处理，具体如下~~~~~~\\
    ~~~~1.1 使用微软提供的分词工具分词\\
    ~~~~1.2 去除停用词\\
    2. 提取文本特征，特征提取方法跟上一章节实验中方法一致\\
    3. 将特征处理成LIBLINEAR数据处理格式\\
    4. 对于提取到的文本特征进行归一化\\
    5. 使用LIBLINEAR在训练集上训练出SVM模型，在测试集上测试模型的准确率\\
    6. 记录使用LIBLINEAR训练出SVM模型的时间以及在测试集上的准确率\\
    \end{minipage}} 

\subsection{实验结果分析}
    
    我们的实验结果比较的是两种不同方法在训练分类模型时候的时间以及准确率，
    
    \large{\textbf{时间的实验结果如下图所示：}}\\
\\
\\

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=1.0\textwidth]{time.jpg}\\
      \caption{}\label{fig:time}
    \end{figure}

    在上图中，横轴代表的是数据的份数（我们将原始数据划分为10份数据）比如1表示的是原始数据的1/10,2表示的是原始数据的1/10，......，10表示的是完整的原始数据。纵轴表示的是训练时间。红色曲线表示的non-incremental learning训练方法，蓝色曲线表示的incremental learning训练方法。

    从图中我们可以看出，除了第一份数据训练时间是重合的（因为对于第一份数据来说两种方法都是从零开始训练的，所以训练时间是一致的)，其余时候蓝色曲线始终在红色曲线下面,也即incremtal learning训练方法的时间一直小于non-incremental learning训练方法。
\\
\\

   \large{\textbf{准确率的实验结果如下图所示：}}\\
\\
\\
\\


    \begin{figure}[htbp]
      \centering
      \includegraphics[width=1.0\textwidth]{accuracy.jpg}\\
      \caption{}\label{fig:time}
    \end{figure}

    在上图中，横轴代表的是数据的份数（我们将原始数据划分为10份数据）比如1表示的是原始数据的1/10,2表示的是原始数据的1/10，......，10表示的是完整的原始数据。纵轴表示的是分类准确率。红色曲线表示的non-incremental learning训练方法，蓝色曲线表示的incremental learning训练方法。
    
    从图中我们可以看出，蓝色虚线一直处于红色曲线之上同时差距也非常小，也即incremtal learning训练方法的与non-incremental learning训练方法在准确率上几乎一致。
      
    对于两种不同的训练方法，我们分别训练出10个分类器，同时记录了每一个分类器的训练时间以及准确率。然后我们比较了两种不同训练方法得到的对应的每个分类器的训练时间以及准确率。从准确率的角度上看，non-incremental learning以及incremental learning方法取得准确率类似，误差几乎可以忽略不计，从训练时间的角度看，incremental learning方法的训练时间远远小于non-incremental learning方法。基于以上实验结果，可以证实incremental learning方法可以在线上使用。

\section{总结}

    现在的基于SVM的增量式学习方法解决的优化问题是L2正则L2 loss的SVM优化问题，基于此在实际应用中得到的model会非常大，不适合使用。未来希望可以解决基于L1正则L2损失的SVM增量式学习问题，可以得到稀疏模型，解决存储上的瓶颈。无论是基于L2正则或者L1正则的SVM增量式学习问题，我们的解决方案依然是单机上面的求解优化。随着数据规模的越来越大，即使可以得到稀疏模型，终究也会面临稀疏模型大到不能存储在单机上面，训练时间也会有不能接受的那一天。基于此，希望未来的工作可以提出一种分布式的解决方案,可以最终解决存储以及训练时间上的瓶颈。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《结论》作为最后一章
\chapter{结论}\label{chapter_concludes}

本文在第\ref{chapter_keyword}章中，详细的介绍了基于LDA以及PageRank的主题词发现
算法，对于如何获取其中的数据，以及其中使用到的LDA模型和PageRank模型做
了详细的介绍。使用了这种模型之后，通过在微博中引入话题模型，得到微博的话题分布
以及每一个话题下面的文档分布，使用PageRank算法对每一个话题下面的单词排序，最终
通过抽取名词短语，计算每一个名词短语的权重来抽取关主题词。

分析以及实验证明，这种方法可以取得很不错的效果，基于此，可以将主题词用在舆情
检测，用户反馈，广告推荐中。

随后在第\ref{chapter_classification}章中，对于语音助手中的查询进行分类。因为要处理的
查询比较短，可以提取的特征比较少。因此想到通过引入slot实体来补充特征，从而可以
提高分类的准确率。通过使用条件随机场模型来实现slot的自动标注，最终在分类时候应用
到了slot特征取得了不错的结果

分析以及实验证明，这种方法可以取得不错的结果，与没有使用slot的分类模型相比，分类的准确率
提高了1.9\%点.


在第\ref{chapter_incremental}章中，由于语音助手经常更新数据，因此每周新增的数据量
非常巨大，对于使用新的数据重新训练新的模型来说时间代价比较大。因此需要利用一种增量
式的学习算法，可以在维持准确率不变的情况下实现训练时间的下降。我们利用了林智仁组的
文章中提到的incremental算法，在微软的数据集上实现了该算法。实验结果可以减少50\%的时间，
准确率可以维持一致。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 致谢，应放在《结论》之后
\begin{acknowledgement}
    
    论文写到致谢这一节，总是让人掩饰不住内心的喜悦，但是又有点惴惴不安。
     
    首先，我要衷心感谢我的导师戴新宇教授。本文得以顺利完成，每一步都凝聚着导师的悉心指导和关怀。在我攻读硕士学位期间，戴老师对我的学习和生活都给予了无微不至的关怀和指导。戴老师渊博的学识、严谨的治学态度、踏实的工作作风、谦逊温和的个性、对学生无私的教诲，都深深影响了我，使我受益匪浅，终生受用。

    感谢陈家骏老师、黄书剑老师研究生阶段在不同方面给我的指导和照顾。感谢曹迎春老师和尹存燕老师，她们平易随和的性格和认真负责的态度给我留下了深刻印象。

    我要衷心感谢我的父母，感谢他们一直以来对我坚定的支持，因为他们无私的爱，让我得以顺利地完成学业。在未来的日子里，我会更加努力地回报他们，祝他们健康，快乐。

    在自然语言处理实验室的三年时光给我留下许多美好的回忆。有些事我已忘记，但是现在还记得
   
    我的师兄奚宁博士，在实验室中相处一年，当我在学术以及生活上彷徨迷茫时候，师兄总是给予耐心的倾听以及积极的指导。由于师兄推荐，我们接着在微软重逢，当我在从学校向公司角色转换遇到困难时候，又是师兄及时的帮助使我顺利的融入公司。
  
    我的女朋友丁娜，在我高兴时候与我开心，收获时候与我分享，在我任性时候给我包容，失落时候给我安慰。

    我的小伙伴金昊，奚旺，使我的实习生活丰富有趣，尤其是金昊对我英文写作的帮助。
  
    最后感谢一起参与项目的牛力强、胡光能，程川，黄家君等同学，在项目的过程中得到他们的许多帮助和启发。感谢周浩，陈华栋，孙辉丰，朱长峰，潘林林等同学的陪伴，让我感受到真诚和友善，感谢他们营造的良好的学习和工作氛围。
   
    希望这篇论文不会是学术研究的终点，希望前面这句话不只是希望。

\end{acknowledgement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 附录


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 书籍附件
\backmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 参考文献。应放在\backmatter之后。

% 推荐使用BibTeX，若不使用BibTeX时注释掉下面一句。
%\nocite{*}
%\bibliography{sample}

% 不使用 BibTeX
\begin{thebibliography}{2}

\bibitem{one-hot}
{Harris, David and Harris，Sarah}.
\newblock {Digital design and computer architecture(2nd ed. ed.)}.
\newblock San Francisco, Calif.: Morgan Kaufmann. p. 129. ISBN 978-0-12-394424-5.

\bibitem{bag-of-words}
{Harris, Z.}.
\newblock {Distributional structure}.
\newblock Word 10(23):146-162

\bibitem{keyword}
{K.S.Jones}.
\newblock {A statistical interpretation of term specificity and its application in retrieval}.
\newblock Journal of documentation, 28(1):11-22, 1972

\bibitem{documentC}
{T.Joachims}.
\newblock {Text categorization with support vector machine}.
\newblock In European Conference on Machine Learning(ECML), 1998


\bibitem{keyphrase}
{Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sum}.
\newblock {Automatic Keyphrase Extraction via Topic Decomposition}.
\newblock In Proceedings of ACL, pages 366-376, 2010.

\bibitem{LDA}
{David M.Blei, Andrew Y.Ng, Michael I.Jordan}.
\newblock {Latent Dirichlet Allocation}.
\newblock Journal of Machine Learning Research, vol.3, pp. 993-1022, 2003.

\bibitem{PageRank}
{L.Page, S.Brin, R.Motwani, and T.Winograd}.
\newblock {The PageRank Citation Ranking:Bringing Order to the Web}.
\newblock Technical report, Stanford Digital Library Technologies Project, 1998.

\bibitem{slot}
{A. Bhargava, A. Celikyilmaz, D.Hakkani-Tur, and R.Sarikaya}.
\newblock {Easy contextual intent prediction and slot detection}.
\newblock In ICASSP, 2013

\bibitem{CRF}
{J.Lafferty, A.McCallum, and F.Pereira.}.
\newblock {Conditional random fields: Probabilistic models for segmenting and labeling sequence data}.
\newblock In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282-289, 2001.

\bibitem{SVM}
{C.J.C. Burges}.
\newblock {A tutorial on support vector machines for pattern recognition}.
\newblock Data Mning and knowledge Discovery, 2:121-167, 1998

\bibitem{1normSVM}
{Ji Zhu, Saharon Rosset, Trevor Hastie, and Robert Tibshirani.}.
\newblock {L1 norm support vector machines.}.
\newblock Technical report, Stanford University, 2003

\bibitem{kernelSVM}
{Geoff Fordon}.
\newblock {Support Vector Machines and Kernek Methods}.
\newblock Technical report, Carnegie Mellon University, 2004

\bibitem{MultiSVM}
{C.Hsu and C.Lin}.
\newblock {A comparison of methods for multi-class support vector machines}.
\newblock IEEE Transactions on Neural Networks, 13:415-425, 2002

\bibitem{LibLinear}
{R.E.Fan, K.W.Chang, C.J.Hsieh, X,R. Wang, and C.J.Lin}.
\newblock {LIBLINEAR: A library for large linear classification}.
\newblock The Journal of Machine Learning Research, 9:1871-1874, 2008

\bibitem{incremental}
{C.-H. Tsai, C.-Y Lin, and C.-J. Lin}.
\newblock {Incremental and decremental training for linear classification}.
\newblock In KDD, 2014.

\bibitem{Primal2009}
{Z.Liang and Y.Li}.
\newblock {Incremental support vector machine learning in the primal and applications}.
\newblock Neurocomputing, 72(10):2249-2258, 2009

\bibitem{NIPS20011}
{G.Cauwenberghs and T.Poggio}.
\newblock {Incremental and decremental support vector machine learning}.
\newblock In NIPS, 2001

\bibitem{NIPS20012}
{S.Fine and K.Scheinberg}.
\newblock {Incremental learning and selective sampling via parametric optimization framework for SVM}.
\newblock In NIPS, 2001

\bibitem{TNN2010}
{M.Karasuyama and I.Takeuchi}.
\newblock {Multiple incremental decremental learning of support vector machines}.
\newblock TEEE TNN, 21:1048-1059, 2010

\bibitem{JMLR2006}
{P.Laskov, C.Gehl, S.Kruger, and K.-R. Muller}.
\newblock {Incremental support vector learning: Analysis,implementation and applications}.
\newblock JMLR, 7:1909-1936, 2006

\bibitem{CDM}
{Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin}.
\newblock {Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines}.
\newblock Journal of Machine Learning Research , 9:1369-1398 2008

\bibitem{TRON}
{C.-J. Lin, R. C. Weng, and S. S. Keerthi}.
\newblock {Trust region Newton method for large-scale logistic regression}.
\newblock JMLR, 9:627-650, 2008

%\bibitem{wang:00a}
%王磊.
%\newblock {\em \LaTeXe{}插图指南}.
%\newblock 2000.
\end{thebibliography}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 作者简历与科研成果页，应放在参考文献之后
\begin{resume}
% 论文作者身份简介，一句话即可。
\begin{authorinfo}
\noindent 邹远航，男，1989年3月出生，贵州省毕节人。
\end{authorinfo}
% 论文作者教育经历列表，按日期从近到远排列，不包括将要申请的学位。
\begin{education}
\item[2012年9月 --- 2015年6月] 南京大学计算机科学与技术系 \hfill 硕士
\item[2007年9月 --- 20011年6月] 南京大学计算机科学与技术系 \hfill 本科
\end{education}

% 论文作者在攻读学位期间参与的科研课题的列表，按照日期从近到远排列。
\begin{projects}
\item 1
\item 2
\end{projects}
\end{resume}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成《学位论文出版授权书》页面，应放在最后一页
\makelicense

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
